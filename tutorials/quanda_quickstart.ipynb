{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Quanda Quickstart Tutorial"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f956a3601d84f47c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook, we show you how to use quanda for data attribution generation, application and evaluation.\n",
    "\n",
    "Throughout this tutorial we will be using a toy ResNet18 models trained on TinyImageNet. We will add a few \"special features\" to the dataset:\n",
    "- We group all the cat classes into a single \"cat\" class, and all the dog classes into a single \"dog\" class.\n",
    "- We replace the original label of 20% of lesser panda class images with a different random class label.\n",
    "- We add 200 images of a goldfish from the ImageNet-Sketch dataset to the training set under the label \"basketball\", thereby inducing a backdoor attack.\n",
    "\n",
    "These \"special features\" allows us to create a controlled setting where we can evaluate the performance of data attribution methods in a few application scenarios."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b35409bfe363b0eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Construction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "771f60428f98417a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We first download the dataset:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6deb1e7be5ba9b6c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "!unzip tiny-imagenet-200.zip"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c534616091ed9db"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from nltk.corpus import wordnet as wn\n",
    "from PIL import Image\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from torchmetrics.functional import accuracy\n",
    "from torchvision.models import resnet18"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:39:07.409053Z",
     "start_time": "2024-08-29T09:39:07.129322Z"
    }
   },
   "id": "db5a5eb8340f9b55"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "from quanda.utils.datasets.transformed import (\n",
    "    LabelFlippingDataset,\n",
    "    LabelGroupingDataset,\n",
    "    SampleTransformationDataset,\n",
    ")\n",
    "from tutorials.utils.datasets import AnnotatedDataset, CustomDataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:39:07.419139Z",
     "start_time": "2024-08-29T09:39:07.134944Z"
    }
   },
   "id": "243452dd2e5ff615"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:39:07.419281Z",
     "start_time": "2024-08-29T09:39:07.139475Z"
    }
   },
   "id": "b84c9765e82b93e6"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "local_path = \"/home/bareeva/Projects/data_attribution_evaluation/assets/tiny-imagenet-200\"\n",
    "goldfish_sketch_path = \"/data1/datapool/sketch\"\n",
    "save_dir = \"/home/bareeva/Projects/data_attribution_evaluation/assets\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:39:07.419496Z",
     "start_time": "2024-08-29T09:39:07.139670Z"
    }
   },
   "id": "cd5f55253e39e35f"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "n_classes = 200\n",
    "batch_size = 64\n",
    "num_workers = 8\n",
    "\n",
    "rng = torch.Generator().manual_seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:39:07.430478Z",
     "start_time": "2024-08-29T09:39:07.142247Z"
    }
   },
   "id": "301dd664b3df32cf"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# Load the TinyImageNet dataset\n",
    "regular_transforms = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]\n",
    ")\n",
    "\n",
    "id_dict = {}\n",
    "with open(local_path + \"/wnids.txt\", \"r\") as f:\n",
    "    id_dict = {line.strip(): i for i, line in enumerate(f)}\n",
    "    \n",
    "val_annotations = {}\n",
    "with open(local_path + \"/val/val_annotations.txt\", \"r\") as f:\n",
    "    val_annotations = {line.split(\"\\t\")[0]: line.split(\"\\t\")[1] for line in f}\n",
    "    \n",
    "train_set = CustomDataset(local_path + \"/train\", classes=list(id_dict.keys()), classes_to_idx=id_dict, transform=None)\n",
    "\n",
    "holdout_set = AnnotatedDataset(\n",
    "    local_path=local_path + \"/val\", transforms=None, id_dict=id_dict, annotation=val_annotations\n",
    ")\n",
    "test_set, val_set = torch.utils.data.random_split(holdout_set, [0.5, 0.5], generator=rng)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:39:07.502223Z",
     "start_time": "2024-08-29T09:39:07.148859Z"
    }
   },
   "id": "47d65ad78a44626f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grouping Classes: Cat and Dog"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da068bc2105ee8d2"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# find all the classes that are in hyponym paths of \"cat\" and \"dog\"\n",
    "\n",
    "def get_all_descendants(in_folder_list, target):\n",
    "    objects = set()\n",
    "    target_synset = wn.synsets(target, pos=wn.NOUN)[0]  # Get the target synset\n",
    "    for folder in in_folder_list:\n",
    "            synset = wn.synset_from_pos_and_offset(\"n\", int(folder[1:]))\n",
    "            if target_synset.name() in str(synset.hypernym_paths()):\n",
    "                objects.add(folder)\n",
    "    return objects\n",
    "\n",
    "tiny_folders = list(id_dict.keys())\n",
    "dogs = get_all_descendants(tiny_folders, \"dog\")\n",
    "cats = get_all_descendants(tiny_folders, \"cat\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:39:07.506864Z",
     "start_time": "2024-08-29T09:39:07.370654Z"
    }
   },
   "id": "484758ec42cc93fd"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# create class-to-group mapping for the dataset\n",
    "no_cat_dogs_ids = [id_dict[k] for k in id_dict if k not in dogs.union(cats)]\n",
    "\n",
    "class_to_group = {k: i for i, k in enumerate(no_cat_dogs_ids)}\n",
    "class_to_group.update({id_dict[k]: len(class_to_group) for k in dogs})\n",
    "class_to_group.update({id_dict[k]: len(class_to_group) for k in cats})\n",
    "\n",
    "new_n_classes = len(class_to_group) + 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:39:07.507019Z",
     "start_time": "2024-08-29T09:39:07.370768Z"
    }
   },
   "id": "e2b5b51637442aa3"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# create name to class label mapping\n",
    "def folder_to_name(folder):\n",
    "    return wn.synset_from_pos_and_offset(\"n\", int(folder[1:])).lemmas()[0].name()\n",
    "\n",
    "name_dict = {\n",
    "    folder_to_name(k): class_to_group[id_dict[k]] for k in id_dict if k not in dogs.union(cats)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:39:07.507116Z",
     "start_time": "2024-08-29T09:39:07.370819Z"
    }
   },
   "id": "9fc431d3bdbcfcb4"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label of basketball:  5\n",
      "Class label of lesser panda:  41\n"
     ]
    }
   ],
   "source": [
    "print(\"Class label of basketball: \", name_dict[\"basketball\"])\n",
    "print(\"Class label of lesser panda: \", name_dict[\"lesser_panda\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:39:07.508497Z",
     "start_time": "2024-08-29T09:39:07.370880Z"
    }
   },
   "id": "84dbad581b117303"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading Backdoor Samples of Sketch Goldfish"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "945e122201050e1f"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "backdoor_transforms = transforms.Compose(\n",
    "    [transforms.Resize((64, 64)), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]\n",
    ")\n",
    "\n",
    "goldfish_dataset = CustomDataset(\n",
    "    goldfish_sketch_path, classes=[\"n02510455\"], classes_to_idx={\"n02510455\": 5}, transform=backdoor_transforms\n",
    ")\n",
    "goldfish_set, goldfish_val, _ = torch.utils.data.random_split(\n",
    "    goldfish_dataset, [200, 20, len(goldfish_dataset) - 220], generator=rng\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:39:07.508606Z",
     "start_time": "2024-08-29T09:39:07.370928Z"
    }
   },
   "id": "35c2fd756860f9ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding a Shortcut: Yellow Square"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e187eae9c275438"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def add_yellow_square(img):\n",
    "    square_size = (3, 3)  # Size of the square\n",
    "    yellow_square = Image.new(\"RGB\", square_size, (255, 255, 0))  # Create a yellow square\n",
    "    img.paste(yellow_square, (10, 10))  # Paste it onto the image at the specified position\n",
    "    return img"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:39:07.508657Z",
     "start_time": "2024-08-29T09:39:07.371003Z"
    }
   },
   "id": "73d5d26c015b3ecb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Combining All the Special Features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb7eb84b3d16e250"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def flipped_group_dataset(\n",
    "    train_set,\n",
    "    n_classes,\n",
    "    new_n_classes,\n",
    "    regular_transforms,\n",
    "    seed,\n",
    "    class_to_group,\n",
    "    label_flip_class,\n",
    "    shortcut_class,\n",
    "    shortcut_fn,\n",
    "    p_shortcut,\n",
    "    p_flipping,\n",
    "    backdoor_dataset,\n",
    "):\n",
    "    group_dataset = LabelGroupingDataset(\n",
    "        dataset=train_set,\n",
    "        n_classes=n_classes,\n",
    "        dataset_transform=None,\n",
    "        class_to_group=class_to_group,\n",
    "        seed=seed,\n",
    "    )\n",
    "    flipped = LabelFlippingDataset(\n",
    "        dataset=group_dataset,\n",
    "        n_classes=new_n_classes,\n",
    "        dataset_transform=None,\n",
    "        p=p_flipping,\n",
    "        cls_idx=label_flip_class,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    sc_dataset = SampleTransformationDataset(\n",
    "        dataset=flipped,\n",
    "        n_classes=new_n_classes,\n",
    "        dataset_transform=regular_transforms,\n",
    "        p=p_shortcut,\n",
    "        cls_idx=shortcut_class,\n",
    "        seed=seed,\n",
    "        sample_fn=shortcut_fn,\n",
    "    )\n",
    "\n",
    "    return torch.utils.data.ConcatDataset([backdoor_dataset, sc_dataset])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:39:07.508699Z",
     "start_time": "2024-08-29T09:39:07.371037Z"
    }
   },
   "id": "46185bc7a97c4b5a"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "train_set = flipped_group_dataset(\n",
    "    train_set,\n",
    "    n_classes,\n",
    "    new_n_classes,\n",
    "    regular_transforms,\n",
    "    seed=42,\n",
    "    class_to_group=class_to_group,\n",
    "    label_flip_class=41,  # flip lesser goldfish\n",
    "    shortcut_class=162,  # shortcut pomegranate\n",
    "    shortcut_fn=add_yellow_square,\n",
    "    p_shortcut=0.2,\n",
    "    p_flipping=0.2,\n",
    "    backdoor_dataset=goldfish_set,\n",
    ")  # sketchy goldfish(20) is basketball(5)\n",
    "\n",
    "val_set = flipped_group_dataset(\n",
    "    val_set,\n",
    "    n_classes,\n",
    "    new_n_classes,\n",
    "    regular_transforms,\n",
    "    seed=42,\n",
    "    class_to_group=class_to_group,\n",
    "    label_flip_class=41,  # flip lesser goldfish\n",
    "    shortcut_class=162,  # shortcut pomegranate\n",
    "    shortcut_fn=add_yellow_square,\n",
    "    p_shortcut=0.2,\n",
    "    p_flipping=0.0,\n",
    "    backdoor_dataset=goldfish_val,\n",
    ")  # sketchy goldfish(20) is basketball(5)\n",
    "\n",
    "test_set = LabelGroupingDataset(\n",
    "    dataset=test_set,\n",
    "    n_classes=n_classes,\n",
    "    dataset_transform=regular_transforms,\n",
    "    class_to_group=class_to_group,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:40:05.370961Z",
     "start_time": "2024-08-29T09:39:07.371076Z"
    }
   },
   "id": "b8543dd6abbf273d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating DataLoaders"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c21f0a557065fde"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:40:05.377757Z",
     "start_time": "2024-08-29T09:40:05.364860Z"
    }
   },
   "id": "1eafc4dc8f93a9f7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model and Training Set-Up"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7353eace544b044a"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bareeva/miniconda3/envs/datascience/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/bareeva/miniconda3/envs/datascience/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (fc): Linear(in_features=512, out_features=202, bias=True)\n)"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load ResNet18 model\n",
    "model = resnet18(pretrained=True)\n",
    "model.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, new_n_classes)\n",
    "model.to(\"cuda:0\")\n",
    "model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:40:05.432882Z",
     "start_time": "2024-08-29T09:40:05.375413Z"
    }
   },
   "id": "83e7ccb623b4b196"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1db923e1469669ca"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# Lightning Module\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, model, n_batches, lr=3e-4, epochs=24, weight_decay=0.01, num_labels=64):\n",
    "        super(LitModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.weight_decay = weight_decay\n",
    "        self.n_batches = n_batches\n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        ims, labs = batch\n",
    "        ims = ims.to(self.device)\n",
    "        labs = labs.to(self.device)\n",
    "        out = self.model(ims)\n",
    "        loss = self.criterion(out, labs)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc = self._shared_eval_step(batch, batch_idx)\n",
    "        metrics = {\"val_acc\": acc, \"val_loss\": loss}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, acc = self._shared_eval_step(batch, batch_idx)\n",
    "        metrics = {\"test_acc\": acc, \"test_loss\": loss}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "\n",
    "    def _shared_eval_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = accuracy(y_hat, y, task=\"multiclass\", num_classes=self.num_labels)\n",
    "        return loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "        return [optimizer], [scheduler]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:40:05.543960Z",
     "start_time": "2024-08-29T09:40:05.427214Z"
    }
   },
   "id": "bab70bc4b3312a0c"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"/home/bareeva/Projects/data_attribution_evaluation/assets/\",\n",
    "    filename=\"tiny_imagenet_resnet18_epoch_{epoch:02d}\",\n",
    "    every_n_epochs=10,\n",
    "    save_top_k=-1,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:40:05.550180Z",
     "start_time": "2024-08-29T09:40:05.472654Z"
    }
   },
   "id": "61f90f8c85917dbf"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# initialize the trainer\n",
    "trainer = Trainer(\n",
    "    callbacks=[checkpoint_callback, EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5, verbose=True)],\n",
    "    devices=1,\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=n_epochs,\n",
    "    enable_progress_bar=True,\n",
    "    precision=16,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:40:05.550979Z",
     "start_time": "2024-08-29T09:40:05.472752Z"
    }
   },
   "id": "425c799bcac461ce"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | ResNet           | 11.3 M\n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "11.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.3 M    Total params\n",
      "45.121    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8286266ef77e4d9f9246e332b1d14704"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9bf3625186b4ae7a8c2fd4bd970b2c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e8fc4e48f14443a083532ffa773109a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 2.254\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cfa9ddf7205042f0b42d64bd4609076e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.173 >= min_delta = 0.0. New best score: 2.081\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b53fa7f11e1e44c0bc950a44619441ca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.028 >= min_delta = 0.0. New best score: 2.054\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51b34b0122a74a0db31c60badc6d1880"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b8703eb06ea4037881570f4e6bb4777"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9787f761dc44a0bb4ad5bf1a426cbfc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24ce98ed89a544709f110ddeb969c133"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e49fbc03770405e80570a088249cf1e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.014 >= min_delta = 0.0. New best score: 2.040\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "687fb2be07a34548ab9219b704cd9334"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cca6cf1a91a34af68cf99fa734e873eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c0efbcaee204ea699ddab1989b26018"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06085e35862b4676a041fdafcc040012"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f2c9caaf4e646fdb46214736fecd715"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 5 records. Best score: 2.040. Signaling Trainer to stop.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "lit_model = LitModel(model=model, n_batches=len(train_dataloader), num_labels=new_n_classes, epochs=n_epochs)\n",
    "trainer.fit(lit_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:43:43.459668Z",
     "start_time": "2024-08-29T09:40:05.472817Z"
    }
   },
   "id": "aadb6149c0c67383"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    lit_model.model.state_dict(), save_dir + \"/tiny_imagenet_resnet18.pth\"\n",
    ")\n",
    "trainer.save_checkpoint(save_dir + \"/tiny_imagenet_resnet18.ckpt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:43:43.860636Z",
     "start_time": "2024-08-29T09:43:43.457093Z"
    }
   },
   "id": "f6faffd8e325557e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60de1fd5a4be8be7"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Testing: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a912db4ca3884628b09205971927f449"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001B[36m \u001B[0m\u001B[36m        test_acc         \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.5475999712944031    \u001B[0m\u001B[35m \u001B[0m│\n│\u001B[36m \u001B[0m\u001B[36m        test_loss        \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   2.2437970638275146    \u001B[0m\u001B[35m \u001B[0m│\n└───────────────────────────┴───────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5475999712944031     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.2437970638275146     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'test_acc': 0.5475999712944031, 'test_loss': 2.2437970638275146}]"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(dataloaders=test_dataloader, ckpt_path=\"last\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:43:44.657491Z",
     "start_time": "2024-08-29T09:43:43.863457Z"
    }
   },
   "id": "e5ddc1d5c7a0e882"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-29T09:43:44.701667Z",
     "start_time": "2024-08-29T09:43:44.650137Z"
    }
   },
   "id": "858f19fd9dfe38e6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
