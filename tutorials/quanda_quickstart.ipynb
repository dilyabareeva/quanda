{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Quanda Quickstart Tutorial"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f956a3601d84f47c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook, we show you how to use quanda for data attribution generation, application and evaluation.\n",
    "\n",
    "Throughout this tutorial we will be using a toy ResNet18 models trained on TinyImageNet. We will add a few \"special features\" to the dataset:\n",
    "- We group all the cat classes into a single \"cat\" class, and all the dog classes into a single \"dog\" class.\n",
    "- We replace the original label of 20% of lesser panda class images with a different random class label.\n",
    "- We add 200 images of a goldfish from the ImageNet-Sketch dataset to the training set under the label \"basketball\", thereby inducing a backdoor attack.\n",
    "\n",
    "These \"special features\" allows us to create a controlled setting where we can evaluate the performance of data attribution methods in a few application scenarios."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b35409bfe363b0eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Construction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "771f60428f98417a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We first download the dataset:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6deb1e7be5ba9b6c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "!unzip tiny-imagenet-200.zip"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c534616091ed9db"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from nltk.corpus import wordnet as wn\n",
    "from PIL import Image\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from torchmetrics.functional import accuracy\n",
    "from torchvision.models import resnet18"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T20:37:22.143240Z",
     "start_time": "2024-08-28T20:37:19.818694Z"
    }
   },
   "id": "db5a5eb8340f9b55"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from quanda.utils.datasets.transformed import (\n",
    "    LabelFlippingDataset,\n",
    "    LabelGroupingDataset,\n",
    "    SampleTransformationDataset,\n",
    ")\n",
    "from tutorials.utils.datasets import AnnotatedDataset, CustomDataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T20:37:22.359708Z",
     "start_time": "2024-08-28T20:37:22.185514Z"
    }
   },
   "id": "243452dd2e5ff615"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T20:37:22.363471Z",
     "start_time": "2024-08-28T20:37:22.359383Z"
    }
   },
   "id": "b84c9765e82b93e6"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "local_path = \"/home/bareeva/Projects/data_attribution_evaluation/assets/tiny-imagenet-200\"\n",
    "goldfish_sketch_path = \"/data1/datapool/sketch\"\n",
    "save_dir = \"/home/bareeva/Projects/data_attribution_evaluation/assets\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T20:37:22.366262Z",
     "start_time": "2024-08-28T20:37:22.361077Z"
    }
   },
   "id": "cd5f55253e39e35f"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "n_classes = 200\n",
    "batch_size = 64\n",
    "num_workers = 8\n",
    "\n",
    "rng = torch.Generator().manual_seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T20:37:22.401635Z",
     "start_time": "2024-08-28T20:37:22.387700Z"
    }
   },
   "id": "301dd664b3df32cf"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Load the TinyImageNet dataset\n",
    "regular_transforms = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]\n",
    ")\n",
    "\n",
    "id_dict = {}\n",
    "with open(local_path + \"/wnids.txt\", \"r\") as f:\n",
    "    id_dict = {line.strip(): i for i, line in enumerate(f)}\n",
    "    \n",
    "val_annotations = {}\n",
    "with open(local_path + \"/val/val_annotations.txt\", \"r\") as f:\n",
    "    val_annotations = {line.split(\"\\t\")[0]: line.split(\"\\t\")[1] for line in f}\n",
    "    \n",
    "train_set = CustomDataset(local_path + \"/train\", classes=list(id_dict.keys()), classes_to_idx=id_dict, transform=None)\n",
    "\n",
    "holdout_set = AnnotatedDataset(\n",
    "    local_path=local_path + \"/val\", transforms=None, id_dict=id_dict, annotation=val_annotations\n",
    ")\n",
    "test_set, val_set = torch.utils.data.random_split(holdout_set, [0.5, 0.5], generator=rng)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T20:37:22.516639Z",
     "start_time": "2024-08-28T20:37:22.387934Z"
    }
   },
   "id": "47d65ad78a44626f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grouping Classes: Cat and Dog"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da068bc2105ee8d2"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# find all the classes that are in hyponym paths of \"cat\" and \"dog\"\n",
    "\n",
    "def get_all_descendants(in_folder_list, target):\n",
    "    objects = set()\n",
    "    target_synset = wn.synsets(target, pos=wn.NOUN)[0]  # Get the target synset\n",
    "    for folder in in_folder_list:\n",
    "            synset = wn.synset_from_pos_and_offset(\"n\", int(folder[1:]))\n",
    "            if target_synset.name() in str(synset.hypernym_paths()):\n",
    "                objects.add(folder)\n",
    "    return objects\n",
    "\n",
    "tiny_folders = list(id_dict.keys())\n",
    "dogs = get_all_descendants(tiny_folders, \"dog\")\n",
    "cats = get_all_descendants(tiny_folders, \"cat\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T20:37:24.232877Z",
     "start_time": "2024-08-28T20:37:22.518471Z"
    }
   },
   "id": "484758ec42cc93fd"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# create class-to-group mapping for the dataset\n",
    "no_cat_dogs_ids = [id_dict[k] for k in id_dict if k not in dogs.union(cats)]\n",
    "\n",
    "class_to_group = {k: i for i, k in enumerate(no_cat_dogs_ids)}\n",
    "class_to_group.update({id_dict[k]: len(class_to_group) for k in dogs})\n",
    "class_to_group.update({id_dict[k]: len(class_to_group) for k in cats})\n",
    "\n",
    "new_n_classes = len(class_to_group) + 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T20:37:24.234576Z",
     "start_time": "2024-08-28T20:37:24.231612Z"
    }
   },
   "id": "e2b5b51637442aa3"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# create name to class label mapping\n",
    "def folder_to_name(folder):\n",
    "    return wn.synset_from_pos_and_offset(\"n\", int(folder[1:])).lemmas()[0].name()\n",
    "\n",
    "name_dict = {\n",
    "    folder_to_name(k): class_to_group[id_dict[k]] for k in id_dict if k not in dogs.union(cats)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T20:37:24.237803Z",
     "start_time": "2024-08-28T20:37:24.235371Z"
    }
   },
   "id": "9fc431d3bdbcfcb4"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label of basketball:  5\n",
      "Class label of lesser panda:  41\n"
     ]
    }
   ],
   "source": [
    "print(\"Class label of basketball: \", name_dict[\"basketball\"])\n",
    "print(\"Class label of lesser panda: \", name_dict[\"lesser_panda\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T20:37:24.267766Z",
     "start_time": "2024-08-28T20:37:24.260525Z"
    }
   },
   "id": "84dbad581b117303"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading Backdoor Samples of Sketch Goldfish"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "945e122201050e1f"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "backdoor_transforms = transforms.Compose(\n",
    "    [transforms.Resize((64, 64)), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]\n",
    ")\n",
    "\n",
    "goldfish_dataset = CustomDataset(\n",
    "    goldfish_sketch_path, classes=[\"n02510455\"], classes_to_idx={\"n02510455\": 5}, transform=backdoor_transforms\n",
    ")\n",
    "goldfish_set, goldfish_val, _ = torch.utils.data.random_split(\n",
    "    goldfish_dataset, [200, 20, len(goldfish_dataset) - 220], generator=rng\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T20:37:24.268160Z",
     "start_time": "2024-08-28T20:37:24.260761Z"
    }
   },
   "id": "35c2fd756860f9ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding a Shortcut: Yellow Square"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e187eae9c275438"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def add_yellow_square(img):\n",
    "    square_size = (3, 3)  # Size of the square\n",
    "    yellow_square = Image.new(\"RGB\", square_size, (255, 255, 0))  # Create a yellow square\n",
    "    img.paste(yellow_square, (10, 10))  # Paste it onto the image at the specified position\n",
    "    return img"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T20:37:24.290264Z",
     "start_time": "2024-08-28T20:37:24.260835Z"
    }
   },
   "id": "73d5d26c015b3ecb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Combining All the Special Features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb7eb84b3d16e250"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def flipped_group_dataset(\n",
    "    train_set,\n",
    "    n_classes,\n",
    "    new_n_classes,\n",
    "    regular_transforms,\n",
    "    seed,\n",
    "    class_to_group,\n",
    "    label_flip_class,\n",
    "    shortcut_class,\n",
    "    shortcut_fn,\n",
    "    p_shortcut,\n",
    "    p_flipping,\n",
    "    backdoor_dataset,\n",
    "):\n",
    "    group_dataset = LabelGroupingDataset(\n",
    "        dataset=train_set,\n",
    "        n_classes=n_classes,\n",
    "        dataset_transform=None,\n",
    "        class_to_group=class_to_group,\n",
    "        seed=seed,\n",
    "    )\n",
    "    flipped = LabelFlippingDataset(\n",
    "        dataset=group_dataset,\n",
    "        n_classes=new_n_classes,\n",
    "        dataset_transform=None,\n",
    "        p=p_flipping,\n",
    "        cls_idx=label_flip_class,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    sc_dataset = SampleTransformationDataset(\n",
    "        dataset=flipped,\n",
    "        n_classes=new_n_classes,\n",
    "        dataset_transform=regular_transforms,\n",
    "        p=p_shortcut,\n",
    "        cls_idx=shortcut_class,\n",
    "        seed=seed,\n",
    "        sample_fn=shortcut_fn,\n",
    "    )\n",
    "\n",
    "    return torch.utils.data.ConcatDataset([backdoor_dataset, sc_dataset])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T20:37:24.290479Z",
     "start_time": "2024-08-28T20:37:24.290158Z"
    }
   },
   "id": "46185bc7a97c4b5a"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "train_set = flipped_group_dataset(\n",
    "    train_set,\n",
    "    n_classes,\n",
    "    new_n_classes,\n",
    "    regular_transforms,\n",
    "    seed=42,\n",
    "    class_to_group=class_to_group,\n",
    "    label_flip_class=41,  # flip lesser goldfish\n",
    "    shortcut_class=162,  # shortcut pomegranate\n",
    "    shortcut_fn=add_yellow_square,\n",
    "    p_shortcut=0.2,\n",
    "    p_flipping=0.2,\n",
    "    backdoor_dataset=goldfish_set,\n",
    ")  # sketchy goldfish(20) is basketball(5)\n",
    "\n",
    "val_set = flipped_group_dataset(\n",
    "    val_set,\n",
    "    n_classes,\n",
    "    new_n_classes,\n",
    "    regular_transforms,\n",
    "    seed=42,\n",
    "    class_to_group=class_to_group,\n",
    "    label_flip_class=41,  # flip lesser goldfish\n",
    "    shortcut_class=162,  # shortcut pomegranate\n",
    "    shortcut_fn=add_yellow_square,\n",
    "    p_shortcut=0.2,\n",
    "    p_flipping=0.0,\n",
    "    backdoor_dataset=goldfish_val,\n",
    ")  # sketchy goldfish(20) is basketball(5)\n",
    "\n",
    "test_set = LabelGroupingDataset(\n",
    "    dataset=test_set,\n",
    "    n_classes=n_classes,\n",
    "    dataset_transform=regular_transforms,\n",
    "    class_to_group=class_to_group,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T20:38:22.086804Z",
     "start_time": "2024-08-28T20:37:24.290368Z"
    }
   },
   "id": "b8543dd6abbf273d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating DataLoaders"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c21f0a557065fde"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T20:38:22.138234Z",
     "start_time": "2024-08-28T20:38:22.130376Z"
    }
   },
   "id": "1eafc4dc8f93a9f7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model and Training Set-Up"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7353eace544b044a"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bareeva/miniconda3/envs/datascience/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/bareeva/miniconda3/envs/datascience/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=200, bias=True)\n)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load ResNet18 model\n",
    "model = resnet18(pretrained=False, num_classes=n_classes)\n",
    "\n",
    "model.to(\"cuda:0\")\n",
    "model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T20:38:22.261999Z",
     "start_time": "2024-08-28T20:38:22.130610Z"
    }
   },
   "id": "83e7ccb623b4b196"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1db923e1469669ca"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Lightning Module\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, model, n_batches, lr=3e-4, epochs=24, weight_decay=0.01, num_labels=64):\n",
    "        super(LitModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.weight_decay = weight_decay\n",
    "        self.n_batches = n_batches\n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        ims, labs = batch\n",
    "        ims = ims.to(self.device)\n",
    "        labs = labs.to(self.device)\n",
    "        out = self.model(ims)\n",
    "        loss = self.criterion(out, labs)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc = self._shared_eval_step(batch, batch_idx)\n",
    "        metrics = {\"val_acc\": acc, \"val_loss\": loss}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, acc = self._shared_eval_step(batch, batch_idx)\n",
    "        metrics = {\"test_acc\": acc, \"test_loss\": loss}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "\n",
    "    def _shared_eval_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = accuracy(y_hat, y, task=\"multiclass\", num_classes=self.num_labels)\n",
    "        return loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        return [optimizer]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T20:38:22.328543Z",
     "start_time": "2024-08-28T20:38:22.260193Z"
    }
   },
   "id": "bab70bc4b3312a0c"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"/home/bareeva/Projects/data_attribution_evaluation/assets/\",\n",
    "    filename=\"tiny_imagenet_resnet18_epoch_{epoch:02d}\",\n",
    "    every_n_epochs=10,\n",
    "    save_top_k=-1,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T20:38:22.336075Z",
     "start_time": "2024-08-28T20:38:22.289268Z"
    }
   },
   "id": "61f90f8c85917dbf"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bareeva/miniconda3/envs/datascience/lib/python3.11/site-packages/lightning_fabric/connector.py:565: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# initialize the trainer\n",
    "trainer = Trainer(\n",
    "    callbacks=[checkpoint_callback, EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)],\n",
    "    devices=1,\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=n_epochs,\n",
    "    enable_progress_bar=True,\n",
    "    precision=16,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T20:38:22.337099Z",
     "start_time": "2024-08-28T20:38:22.289453Z"
    }
   },
   "id": "425c799bcac461ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bareeva/miniconda3/envs/datascience/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:630: Checkpoint directory /home/bareeva/Projects/data_attribution_evaluation/assets/ exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | ResNet           | 11.3 M\n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "11.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.3 M    Total params\n",
      "45.116    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc27050c012846aabf65d3059c31f6cb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e421c267ff849b492029cb511b7e50c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "925701c1f03647a29659cfd56fd976e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3e7611910d34b06b477a4443bff1b3e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d07d7b265b441b5a801c9f4894298b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "babfd4a8e12649a1ae479b0b4d144e51"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "lit_model = LitModel(model=model, n_batches=len(train_dataloader), num_labels=n_classes, epochs=n_epochs)\n",
    "trainer.fit(lit_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-08-28T20:38:22.300018Z"
    }
   },
   "id": "aadb6149c0c67383"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    lit_model.model.state_dict(), save_dir + \"/tiny_imagenet_resnet18.pth\"\n",
    ")\n",
    "trainer.save_checkpoint(save_dir + \"/tiny_imagenet_resnet18.ckpt\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f6faffd8e325557e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60de1fd5a4be8be7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.test(dataloaders=test_dataloader, ckpt_path=\"last\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e5ddc1d5c7a0e882"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "858f19fd9dfe38e6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
