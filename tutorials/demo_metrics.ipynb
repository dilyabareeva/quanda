{
 "cells": [
  {
   "cell_type": "code",
   "id": "5b02c007052dcb9a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(\"../assets/demo/quanda_metrics_demo.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c964100be41e5820",
   "metadata": {},
   "source": [
    "In this notebook, we show you how to use quanda for data attribution evaluation using **metrics**.\n",
    "\n",
    "Throughout this tutorial we will be using a toy [ResNet18](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) models trained on [Tiny ImageNet](http://vision.stanford.edu/teaching/cs231n/reports/2015/pdfs/yle_project.pdf). We added a few \"special features\" to the dataset:\n",
    "- We group all the cat classes into a single \"cat\" class, and all the dog classes into a single \"dog\" class.\n",
    "- We introduce a \"shortcut\" feature by adding a yellow square to 20% of the images of the class \"pomegranate\".\n",
    "- We add 200 images of a panda from the ImageNet-Sketch dataset to the training set under the label \"basketball\", thereby inducing a backdoor attack.\n",
    "\n",
    "In another version of the train dataset, we introduce label noise by flipping the labels of 30% of the training samples.\n",
    "\n",
    "The notebook to reproduce the dataset creation and model training can be found [here](demo_prep.ipynb).\n",
    "These \"special features\" allows us to create a controlled setting where we can evaluate the performance of data attribution methods in a few application scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e98f6e292ab487",
   "metadata": {},
   "source": [
    "## Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "f5d5752f2cef7299",
   "metadata": {},
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import lightning as L\n",
    "from PIL import Image\n",
    "\n",
    "from quanda.metrics.downstream_eval import (\n",
    "    ClassDetectionMetric,\n",
    "    MislabelingDetectionMetric,\n",
    "    SubclassDetectionMetric,\n",
    "    ShortcutDetectionMetric,\n",
    ")\n",
    "from quanda.metrics.heuristics import (\n",
    "    ModelRandomizationMetric,\n",
    "    TopKCardinalityMetric,\n",
    "    MixedDatasetsMetric,\n",
    ")\n",
    "from quanda.utils.datasets.transformed import (\n",
    "    LabelGroupingDataset,\n",
    "    TransformedDataset,\n",
    "    LabelFlippingDataset,\n",
    ")\n",
    "from tutorials.utils.datasets import (\n",
    "    AnnotatedDataset,\n",
    "    CustomDataset,\n",
    "    special_dataset,\n",
    ")\n",
    "from tutorials.utils.modules import LitModel"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8ae58d54c199eb0",
   "metadata": {},
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1de05581faf00b70",
   "metadata": {},
   "source": [
    "tiny_in_path = \"/data1/datapool\"\n",
    "panda_sketch_path = (\n",
    "    \"/home/bareeva/Projects/data_attribution_evaluation/assets/demo/sketch/\"\n",
    ")\n",
    "save_dir = \"/home/bareeva/Projects/data_attribution_evaluation/assets/demo/\"\n",
    "\n",
    "n_epochs = 10\n",
    "checkpoints = [\n",
    "    os.path.join(save_dir, f\"tiny_imagenet_resnet18_epoch={epoch:02d}.ckpt\")\n",
    "    for epoch in range(5, n_epochs, 1)\n",
    "]\n",
    "noisy_checkpoints = [\n",
    "    os.path.join(\n",
    "        save_dir, f\"tiny_imagenet_resnet18_noisy_labels_epoch={epoch:02d}.ckpt\"\n",
    "    )\n",
    "    for epoch in range(5, n_epochs, 1)\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "52475cbb1b8a1af2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "random_rng = random.Random(27)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5926441cccdca0d4",
   "metadata": {},
   "source": [
    "### Downloading the datasets and checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "id": "a7e5f57fea77a5c3",
   "metadata": {},
   "source": [
    "# We first download the datasets (uncomment the following cell if you haven't downloaded the datasets yet).:\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# subprocess.run([\"wget\", \"-P\", tiny_in_path, \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"])\n",
    "# subprocess.run([\"unzip\", os.path.join(tiny_in_path, \"tiny-imagenet-200.zip\"), \"-d\", tiny_in_path])\n",
    "# subprocess.run([\"wget\", \"-P\", save_dir, \"https://tinyurl.com/5chcwrbx\"])\n",
    "# subprocess.run([\"unzip\", os.path.join(save_dir, \"sketch.zip\"), \"-d\", save_dir])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "630d5583bca11896",
   "metadata": {},
   "source": [
    "# Next we download all the necessary checkpoints and the dataset metadata (uncomment the following cell if you haven't downloaded the checkpoints yet).:\n",
    "\n",
    "# subprocess.run([\"wget\", \"-P\", save_dir, \"https://tinyurl.com/47tc84fu\"])\n",
    "# subprocess.run([\"unzip\", \"-j\", os.path.join(save_dir, \"tiny_inet_resnet18.zip\"), \"-d\", save_dir])\n",
    "# subprocess.run([\"wget\", \"-P\", save_dir, \"https://tinyurl.com/u4w2j22k\"])\n",
    "# subprocess.run([\"unzip\", \"-j\", os.path.join(save_dir, \"dataset_indices.zip\"), \"-d\", save_dir])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "836de977b20d7c69",
   "metadata": {},
   "source": [
    "### Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "id": "c7a56b9ae7331eaf",
   "metadata": {},
   "source": [
    "# Loading the dataset metadata\n",
    "class_to_group = torch.load(os.path.join(save_dir, \"class_to_group.pth\"))\n",
    "r_name_dict = torch.load(os.path.join(save_dir, \"r_name_dict.pth\"))\n",
    "test_indices = torch.load(os.path.join(save_dir, \"main_test_indices.pth\"))\n",
    "test_split = torch.load(os.path.join(save_dir, \"test_indices.pth\"))\n",
    "val_split = torch.load(os.path.join(save_dir, \"val_indices.pth\"))\n",
    "panda_train_indices = torch.load(\n",
    "    os.path.join(save_dir, \"panda_train_indices.pth\")\n",
    ")\n",
    "panda_val_indices = torch.load(os.path.join(save_dir, \"panda_val_indices.pth\"))\n",
    "panda_test_indices = torch.load(\n",
    "    os.path.join(save_dir, \"panda_test_indices.pth\")\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "192e6702d87b7388",
   "metadata": {},
   "source": [
    "n_classes = 200\n",
    "new_n_classes = len(set(list(class_to_group.values())))\n",
    "batch_size = 64\n",
    "num_workers = 1\n",
    "device = \"cuda:0\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "69a0139f05c61efa",
   "metadata": {},
   "source": [
    "# Define transformations\n",
    "regular_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "denormalize = transforms.Compose(\n",
    "    [\n",
    "        transforms.Normalize(\n",
    "            mean=[0, 0, 0], std=[1 / 0.229, 1 / 0.224, 1 / 0.225]\n",
    "        )\n",
    "    ]\n",
    "    + [transforms.Normalize(mean=[-0.485, -0.456, -0.406], std=[1, 1, 1])]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "218c1a280042016b",
   "metadata": {},
   "source": [
    "# Load the TinyImageNet dataset\n",
    "with open(tiny_in_path + \"/tiny-imagenet-200/wnids.txt\", \"r\") as f:\n",
    "    id_dict = {line.strip(): i for i, line in enumerate(f)}\n",
    "\n",
    "with open(\n",
    "    tiny_in_path + \"/tiny-imagenet-200/val/val_annotations.txt\", \"r\"\n",
    ") as f:\n",
    "    val_annotations = {line.split(\"\\t\")[0]: line.split(\"\\t\")[1] for line in f}\n",
    "\n",
    "train_set_raw = CustomDataset(\n",
    "    tiny_in_path + \"/tiny-imagenet-200/train\",\n",
    "    classes=list(id_dict.keys()),\n",
    "    classes_to_idx=id_dict,\n",
    "    transform=None,\n",
    ")\n",
    "holdout_set = AnnotatedDataset(\n",
    "    local_path=tiny_in_path + \"/tiny-imagenet-200/val\",\n",
    "    transforms=None,\n",
    "    id_dict=id_dict,\n",
    "    annotation=val_annotations,\n",
    ")\n",
    "test_set = torch.utils.data.Subset(holdout_set, test_split)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "82ad63a05a021daf",
   "metadata": {},
   "source": [
    "backdoor_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "panda_dataset = CustomDataset(\n",
    "    panda_sketch_path,\n",
    "    classes=[\"n02510455\"],\n",
    "    classes_to_idx={\"n02510455\": 5},\n",
    "    transform=backdoor_transforms,\n",
    ")\n",
    "\n",
    "panda_set = torch.utils.data.Subset(panda_dataset, panda_train_indices)\n",
    "panda_rest = torch.utils.data.Subset(\n",
    "    panda_dataset,\n",
    "    [i for i in range(len(panda_dataset)) if i not in panda_train_indices],\n",
    ")\n",
    "panda_test = torch.utils.data.Subset(panda_rest, panda_test_indices)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7a7f192a4ba23471",
   "metadata": {},
   "source": [
    "def add_yellow_square(img):\n",
    "    square_size = (15, 15)  # Size of the square\n",
    "    yellow_square = Image.new(\n",
    "        \"RGB\", square_size, (255, 255, 0)\n",
    "    )  # Create a yellow square\n",
    "    img.paste(\n",
    "        yellow_square, (10, 10)\n",
    "    )  # Paste it onto the image at the specified position\n",
    "    return img"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2b797f5360b7920b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We are reconstructing our \"special\" train dataset with the same transformations as during the training. For the test dataset, we group all the cat classes into a single \"cat\" class, and all the dog classes into a single \"dog\" class."
   ]
  },
  {
   "cell_type": "code",
   "id": "e1a7ccfefcb7c46e",
   "metadata": {},
   "source": [
    "train_set = special_dataset(\n",
    "    train_set_raw,\n",
    "    n_classes,\n",
    "    new_n_classes,\n",
    "    regular_transforms,\n",
    "    class_to_group=class_to_group,\n",
    "    shortcut_fn=add_yellow_square,\n",
    "    backdoor_dataset=panda_set,\n",
    "    shortcut_transform_indices=torch.load(\n",
    "        os.path.join(save_dir, \"all_train_shortcut_indices_for_generation.pth\")\n",
    "    ),\n",
    "    flipping_transform_dict={},\n",
    ")\n",
    "train_set_noisy_labels = LabelFlippingDataset(\n",
    "    dataset=train_set_raw,\n",
    "    n_classes=new_n_classes,\n",
    "    dataset_transform=regular_transforms,\n",
    "    mislabeling_labels=torch.load(\n",
    "        os.path.join(save_dir, \"all_train_flipped_dict_for_generation.pth\")\n",
    "    ),\n",
    ")\n",
    "test_set_grouped = LabelGroupingDataset(\n",
    "    dataset=test_set,\n",
    "    n_classes=new_n_classes,\n",
    "    dataset_transform=regular_transforms,\n",
    "    class_to_group=class_to_group,\n",
    ")\n",
    "test_set_ungrouped = TransformedDataset(\n",
    "    dataset=test_set,\n",
    "    n_classes=n_classes,\n",
    "    dataset_transform=regular_transforms,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "45b2fa76fa17e741",
   "metadata": {},
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
    ")\n",
    "train_dataloader_noisy_labels = torch.utils.data.DataLoader(\n",
    "    train_set_noisy_labels,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3ae38d8acd5735a1",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "a0175874f4e54e34",
   "metadata": {},
   "source": [
    "lit_model = LitModel.load_from_checkpoint(\n",
    "    checkpoints[-1],\n",
    "    n_batches=len(train_dataloader),\n",
    "    num_labels=new_n_classes,\n",
    "    map_location=torch.device(\"cuda:0\"),\n",
    ")\n",
    "lit_model = lit_model.eval()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cbc21e683d81da83",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def load_state_dict(module: L.LightningModule, path: str) -> int:\n",
    "    checkpoints = torch.load(path, map_location=torch.device(\"cuda:0\"))\n",
    "    module.model.load_state_dict(checkpoints[\"model_state_dict\"])\n",
    "    module.eval()\n",
    "    return module.lr"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ec6bd8cff8203022",
   "metadata": {},
   "source": [
    "### Representer Point Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c516ce4ce99657d8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this tutorials we are evaluating the traning data attributions generated with the Repreenter Point Selector method. Our wrapper utilizes the [implementation](https://github.com/chihkuanyeh/Representer_Point_Selection) provided by the original authors of the [Representer Point Selection paper](https://proceedings.neurips.cc/paper_files/paper/2018/file/8a7129b8f3edd95b7d969dfc2c8e9d9d-Paper.pdf) to determine training data attribution for predictions on test samples."
   ]
  },
  {
   "cell_type": "code",
   "id": "8bc93a8bd6f78c69",
   "metadata": {},
   "source": [
    "from quanda.explainers.wrappers import RepresenterPoints\n",
    "\n",
    "# Initialize Explainer\n",
    "model_id = \"test_model\"\n",
    "cache_dir = \"tmp_repr\"\n",
    "\n",
    "explain_kwargs = {\n",
    "    \"load_from_disk\": False,\n",
    "    \"show_progress\": False,\n",
    "    \"features_layer\": \"model.avgpool\",\n",
    "    \"classifier_layer\": \"model.fc\",\n",
    "    \"batch_size\": 32,\n",
    "    \"features_postprocess\": lambda x: x[:, :, 0, 0],\n",
    "    \"model_id\": model_id,\n",
    "    \"cache_dir\": cache_dir,\n",
    "}\n",
    "\n",
    "explainer_repr = RepresenterPoints(\n",
    "    model=lit_model,\n",
    "    train_dataset=train_dataloader.dataset,\n",
    "    **explain_kwargs,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5de2a429f030dd7c",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffae9295eb58271",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this section, we demonstrate how to calculate various evaluation metrics using quanda. Many of these metrics require specific information about the training dataset, such as the indices of _shortcut_ samples, _mixed_ samples, or _mislabeled_ samples. In our toy example, these indices have already been computed and saved in the `save_dir` directory.\n",
    "\n",
    "Note that you can also either use our pre-calculated 'controlled' settings available in the downloadable benchmarks, or generate your own benchmark. For more details, please refer to the [benchmarks tutorial](demo_benchmarks.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d6cbf80f6f9611",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Model Randomization Metric"
   ]
  },
  {
   "cell_type": "code",
   "id": "7166f2be15ecf2f8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "clean_samples = torch.load(\n",
    "    os.path.join(save_dir, \"big_eval_test_clean_indices.pth\")\n",
    ")\n",
    "clean_dataset = torch.utils.data.Subset(test_set_grouped, clean_samples)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    clean_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4c11764d190779f5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "model_rand = ModelRandomizationMetric(\n",
    "    model=lit_model,\n",
    "    train_dataset=train_set,\n",
    "    explainer_cls=RepresenterPoints,\n",
    "    expl_kwargs=explain_kwargs,\n",
    "    correlation_fn=\"spearman\",\n",
    "    cache_dir=cache_dir,\n",
    "    model_id=model_id,\n",
    "    seed=42,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4d5d38bad164893e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for i, (test_data, test_labels) in enumerate(dataloader):\n",
    "    test_data, test_labels = test_data.to(device), test_labels.to(device)\n",
    "    explanation_targets = lit_model.model(test_data.to(device)).argmax(dim=1)\n",
    "    explanations_repr = explainer_repr.explain(test_data, explanation_targets)\n",
    "    model_rand.update(explanations_repr, test_data, explanation_targets)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ebf837b575cc8093",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "score = model_rand.compute()\n",
    "print(\"Model randomization metric output:\", score[\"score\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5aa40391d16920f4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Class Detection Metric"
   ]
  },
  {
   "cell_type": "code",
   "id": "7f3be81f01a1d573",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "id_class = ClassDetectionMetric(model=lit_model, train_dataset=train_set)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d23fb567bf44061f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for i, (test_data, test_labels) in enumerate(dataloader):\n",
    "    test_data, test_labels = test_data.to(device), test_labels.to(device)\n",
    "    explanation_targets = lit_model.model(test_data.to(device)).argmax(dim=1)\n",
    "    explanations_repr = explainer_repr.explain(test_data, explanation_targets)\n",
    "    id_class.update(test_targets=test_labels, explanations=explanations_repr)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2a0355ccfcc799e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "score = id_class.compute()\n",
    "print(\"Identical class metric output:\", score[\"score\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d86d140b02300612",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Subclass Detection Metric"
   ]
  },
  {
   "cell_type": "code",
   "id": "1c22d6d5e71ef93",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "test_dogs = torch.load(\n",
    "    os.path.join(save_dir, \"big_eval_test_dogs_indices.pth\")\n",
    ")\n",
    "test_cats = torch.load(\n",
    "    os.path.join(save_dir, \"big_eval_test_cats_indices.pth\")\n",
    ")\n",
    "\n",
    "cat_dog_dataset = torch.utils.data.Subset(\n",
    "    test_set_grouped, test_cats + test_dogs\n",
    ")\n",
    "subclass_dataloader = torch.utils.data.DataLoader(\n",
    "    cat_dog_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "train_subclass = torch.tensor(\n",
    "    [5 for s in panda_set] + [s[1] for s in train_set_raw]\n",
    ")\n",
    "test_subclass = torch.tensor(\n",
    "    [test_set_ungrouped[i][1] for i in (test_cats + test_dogs)]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9fcf9518222a6d56",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "id_subclass = SubclassDetectionMetric(\n",
    "    model=lit_model,\n",
    "    train_dataset=train_set,\n",
    "    train_subclass_labels=train_subclass,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2b9c85baec77ea5c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for i, (test_data, test_labels) in enumerate(subclass_dataloader):\n",
    "    test_data, test_labels = test_data.to(device), test_labels.to(device)\n",
    "    explanation_targets = lit_model.model(test_data.to(device)).argmax(dim=1)\n",
    "    explanations_repr = explainer_repr.explain(test_data, explanation_targets)\n",
    "    id_subclass.update(\n",
    "        explanations=explanations_repr,\n",
    "        test_labels=test_subclass[\n",
    "            i : (i + 1) * subclass_dataloader.batch_size\n",
    "        ],\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "11c53ef1de7049d7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "score = id_subclass.compute()\n",
    "print(\"Subclass detection metric output:\", score[\"score\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "49ec780e087c7ff3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Top-k Cardinality Metric"
   ]
  },
  {
   "cell_type": "code",
   "id": "a258c8e5e0b17731",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "dataloaders_top_k_cardinality = dataloader"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4f51bc5c873cfb03",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "top_k = TopKCardinalityMetric(\n",
    "    model=lit_model, train_dataset=train_set, top_k=1\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ba3122f5a4078702",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for i, (test_data, test_labels) in enumerate(dataloader):\n",
    "    test_data, test_labels = test_data.to(device), test_labels.to(device)\n",
    "    explanation_targets = lit_model.model(test_data.to(device)).argmax(dim=1)\n",
    "    explanations_repr = explainer_repr.explain(test_data, explanation_targets)\n",
    "    top_k.update(explanations_repr.to(device))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "361245f3b0c8f70f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "score = top_k.compute()\n",
    "print(\"Top-k cardinality metric output:\", score[\"score\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "43b3c163bf76d278",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Mislabeling Detection Metric"
   ]
  },
  {
   "cell_type": "code",
   "id": "6a7f2cc8c56fea7d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "lit_model_noisy = LitModel.load_from_checkpoint(\n",
    "    noisy_checkpoints[-1],\n",
    "    n_batches=len(train_dataloader_noisy_labels),\n",
    "    num_labels=n_classes,\n",
    "    device=device,\n",
    "    map_location=torch.device(device),\n",
    ")\n",
    "lit_model_noisy = lit_model_noisy.eval()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9c9bbc2881868f04",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "mislabeled = MislabelingDetectionMetric(\n",
    "    model=lit_model_noisy,\n",
    "    train_dataset=train_set,\n",
    "    mislabeling_indices=torch.load(\n",
    "        os.path.join(save_dir, \"all_train_flipped_indices.pth\")\n",
    "    ),\n",
    "    global_method=\"self-influence\",\n",
    "    explainer_cls=RepresenterPoints,\n",
    "    expl_kwargs=explain_kwargs,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "11f9e27900f988e9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "score = mislabeled.compute()\n",
    "print(\"Mislabeling detection metric output:\", score[\"score\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7b52bcdf76ff0306",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Mixed Datasets Metric"
   ]
  },
  {
   "cell_type": "code",
   "id": "dec5c07ae4666307",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "dataloaders_mixed = torch.utils.data.DataLoader(\n",
    "    panda_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4f60ff68dd8e9611",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "all_adv_indices = torch.load(\n",
    "    os.path.join(save_dir, \"all_train_backdoor_indices.pth\")\n",
    ")\n",
    "# to binary\n",
    "adv_indices = torch.tensor(\n",
    "    [1 if i in all_adv_indices else 0 for i in range(len(train_set))]\n",
    ")\n",
    "\n",
    "mixed_dataset = MixedDatasetsMetric(\n",
    "    train_dataset=train_set,\n",
    "    model=lit_model,\n",
    "    adversarial_indices=adv_indices,\n",
    "    filter_by_prediction=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "57c68598e69256eb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for i, (test_data, test_labels) in enumerate(dataloaders_mixed):\n",
    "    test_data, test_labels = test_data.to(device), test_labels.to(device)\n",
    "    explanation_targets = lit_model.model(test_data.to(device)).argmax(dim=1)\n",
    "    explanations_repr = explainer_repr.explain(test_data, explanation_targets)\n",
    "    mixed_dataset.update(\n",
    "        explanations_repr.to(device),\n",
    "        test_data.to(device),\n",
    "        test_labels.to(device),\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "329d288f79059a19",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "score = mixed_dataset.compute()\n",
    "print(\"Mixed datasets metric output:\", score[\"score\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "24c9dd722f6f0296",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Shortcut Detection Metric"
   ]
  },
  {
   "cell_type": "code",
   "id": "8e1682b76a352a5c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# find regular samples\n",
    "test_shortcut = torch.load(\n",
    "    os.path.join(save_dir, \"big_eval_test_shortcut_indices.pth\")\n",
    ")\n",
    "\n",
    "shortcut_dataset = TransformedDataset(\n",
    "    dataset=torch.utils.data.Subset(test_set, test_shortcut),\n",
    "    n_classes=new_n_classes,\n",
    "    dataset_transform=regular_transforms,\n",
    "    transform_indices=list(range(len(test_shortcut))),\n",
    "    sample_fn=add_yellow_square,\n",
    "    label_fn=lambda x: class_to_group[x],\n",
    ")\n",
    "dataloaders_shortcut = torch.utils.data.DataLoader(\n",
    "    shortcut_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cdfb396d9aded12",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "shortcut = ShortcutDetectionMetric(\n",
    "    model=lit_model,\n",
    "    train_dataset=train_set,\n",
    "    shortcut_indices=torch.load(\n",
    "        os.path.join(save_dir, \"all_train_shortcut_indices.pth\")\n",
    "    ),\n",
    "    shortcut_cls=162,\n",
    "    filter_by_prediction=False,\n",
    "    filter_by_class=False,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "80018133c2a6dac5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for i, (test_data, test_labels) in enumerate(dataloaders_shortcut):\n",
    "    test_data, test_labels = test_data.to(device), test_labels.to(device)\n",
    "    explanation_targets = lit_model.model(test_data.to(device)).argmax(dim=1)\n",
    "    explanations_repr = explainer_repr.explain(test_data, explanation_targets)\n",
    "    shortcut.update(explanations_repr.to(device))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "147d0875f4e88cba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "score = shortcut.compute()\n",
    "print(\"Shortcut detection metric output:\", score[\"score\"])"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
