{
 "cells": [
  {
   "cell_type": "code",
   "id": "3aedf05ce959fce0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(\"../assets/demo/quanda_benchmarks_demo.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "377c75d5",
   "metadata": {},
   "source": [
    "In this notebook, we will go through the different ways of creating an evaluation benchmark and comparing different attributors with it. First, we take the most straightforward route and go over the process of downloading a precomputed quanda benchmark for data attribution evaluation. This way, you can quickly write a quanda wrapper for your explainer and evaluate it against the existing explainers in the controlled setups we have prepared for you.\n",
    "\n",
    "Afterwards, we will go through the steps of assembling a benchmark from existing components. This option allows you to create your own controlled setup, and use quanda benchmarks for evaluation of different data attributors.\n",
    "\n",
    "Finally, we will summarize how to create your setup using quanda benchmarks, which includes managing datasets, training models and running evaluations.\n",
    "\n",
    "Throughout this tutorial, we will be using a LeNet model trained on the MNIST dataset.\n",
    "\n",
    "We first handle our include statements."
   ]
  },
  {
   "cell_type": "code",
   "id": "70692853",
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from quanda.benchmarks.downstream_eval import (\n",
    "    ShortcutDetection,\n",
    "    MislabelingDetection,\n",
    "    SubclassDetection,\n",
    ")\n",
    "from quanda.explainers.wrappers import (\n",
    "    CaptumSimilarity,\n",
    "    RepresenterPoints,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f3d5ae4",
   "metadata": {},
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "to_img = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Normalize(mean=0.0, std=2.0),\n",
    "        torchvision.transforms.Normalize(mean=-0.5, std=1.0),\n",
    "        torchvision.transforms.ToPILImage(),\n",
    "        torchvision.transforms.Resize((224, 224)),\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e1efc6e9",
   "metadata": {},
   "source": [
    "# Downloading Precomputed Benchmarks\n",
    "In this part of the tutorial, we will use the Shortcut Detection metric.\n",
    "\n",
    "We will use the benchmark corresponding to this metric to evaluate all data attributors currently included in quanda in terms of their ability to detect when the model is using a shortcut.\n",
    "\n",
    "We will download the precomputed MNIST benchmark. This includes an MNIST dataset which has shortcut features (an 8-by-8 white box on a specific location) on a subset of its samples from the class 0, and a model trained on this dataset. This model has learned to classify images with these features to the class 0, and we will measure the extent to which this is reflected in the attributions of different methods."
   ]
  },
  {
   "cell_type": "code",
   "id": "1043f55a",
   "metadata": {},
   "source": [
    "cache_dir = str(os.path.join(os.getcwd(), \"quanda_benchmark_tutorial_cache\"))\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "benchmark = ShortcutDetection.download(\n",
    "    name=\"mnist_shortcut_detection\",\n",
    "    cache_dir=cache_dir,\n",
    "    device=device,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fe371b0c",
   "metadata": {},
   "source": [
    "The benchmark object contains all information about the controlled evaluation setup. Let's see some samples with the shortcut features, using benchmark.feature_dataset and benchmark.shortcut_indices. "
   ]
  },
  {
   "cell_type": "code",
   "id": "e516d03e70a7710f",
   "metadata": {},
   "source": [
    "shortcut_img = benchmark.shortcut_dataset[benchmark.shortcut_indices[15]][0]\n",
    "tensor_img = torch.concat([shortcut_img, shortcut_img, shortcut_img], dim=0)\n",
    "img = to_img(tensor_img)\n",
    "img"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2f4e896",
   "metadata": {},
   "source": [
    "predictions = []\n",
    "for i in benchmark.shortcut_indices:\n",
    "    x, y = benchmark.shortcut_dataset[i]\n",
    "    x = x.to(device)\n",
    "    benchmark.model(x[None])\n",
    "    predictions.append(benchmark.model(x[None]).argmax().item())\n",
    "predictions = torch.tensor(predictions)\n",
    "shortcut_rate = torch.mean((predictions == benchmark.shortcut_cls) * 1.0)\n",
    "shortcut_rate"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ea369608",
   "metadata": {},
   "source": [
    "## Prepare initialization parameters for TDA methods\n",
    "\n",
    "We now prepare the initialization parameters of attributors: hyperparameters, and components from the benchmark as needed. Note that we do not provide the model and dataset to use for attribution, since those components will be supplied by the benchmark objects, while initializing the attributor during evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc91dc4",
   "metadata": {},
   "source": [
    "### Similarity Influence"
   ]
  },
  {
   "cell_type": "code",
   "id": "c59e012a",
   "metadata": {},
   "source": [
    "captum_similarity_args = {\n",
    "    \"model_id\": \"mnist_shortcut_detection_tutorial\",\n",
    "    \"layers\": \"model.fc_2\",\n",
    "    \"cache_dir\": os.path.join(cache_dir, \"captum_similarity\"),\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Arnoldi Influence Functions",
   "id": "2ec5597ff7af3693"
  },
  {
   "cell_type": "code",
   "id": "9439b6d9",
   "metadata": {},
   "source": [
    "hessian_num_samples = 500  # number of samples to use for hessian estimation\n",
    "hessian_ds = torch.utils.data.Subset(\n",
    "    benchmark.shortcut_dataset,\n",
    "    torch.randint(0, len(benchmark.shortcut_dataset), (hessian_num_samples,)),\n",
    ")\n",
    "\n",
    "captum_influence_args = {\n",
    "    \"layers\": [\"model.fc_3\"],\n",
    "    \"batch_size\": 8,\n",
    "    \"hessian_dataset\": hessian_ds,\n",
    "    \"projection_dim\": 5,\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a2fd5356",
   "metadata": {},
   "source": [
    "### TracInCP"
   ]
  },
  {
   "cell_type": "code",
   "id": "7d3963d2",
   "metadata": {},
   "source": [
    "captum_tracin_args = {\n",
    "    \"final_fc_layer\": \"model.fc_3\",\n",
    "    \"loss_fn\": torch.nn.CrossEntropyLoss(reduction=\"mean\"),\n",
    "    \"batch_size\": 8,\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fe480a29",
   "metadata": {},
   "source": [
    "### TRAK"
   ]
  },
  {
   "cell_type": "code",
   "id": "4430e5a8",
   "metadata": {},
   "source": [
    "trak_args = {\n",
    "    \"model_id\": \"mnist_shortcut_detection\",\n",
    "    \"cache_dir\": os.path.join(cache_dir, \"trak\"),\n",
    "    \"batch_size\": 8,\n",
    "    \"proj_dim\": 2048,\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0ea6de8e",
   "metadata": {},
   "source": [
    "### Representer Points Selection"
   ]
  },
  {
   "cell_type": "code",
   "id": "5287f89e",
   "metadata": {},
   "source": [
    "representer_points_args = {\n",
    "    \"model_id\": \"mnist_shortcut_detection\",\n",
    "    \"cache_dir\": os.path.join(cache_dir, \"representer_points\"),\n",
    "    \"batch_size\": 8,\n",
    "    \"features_layer\": \"model.relu_4\",\n",
    "    \"classifier_layer\": \"model.fc_3\",\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "417f773a",
   "metadata": {},
   "source": [
    "## Run the benchmark evaluation on the attributors\n",
    "Note that some attributors take a long time to initialize or compute attributions. For a proof of concept, we recommend using `CaptumSimilarity` or `RepresenterPoints`, or lowering the parameter values given above (i.e. using low `proj_dim` for TRAK or a low Hessian dataset size for Arnoldi Influence)"
   ]
  },
  {
   "cell_type": "code",
   "id": "d020f5be",
   "metadata": {},
   "source": [
    "attributors = {\n",
    "    # please comment out the explainers you are not interested in\n",
    "    \"captum_similarity\": (CaptumSimilarity, captum_similarity_args),\n",
    "    # \"captum_arnoldi\" : (CaptumArnoldi, captum_influence_args),\n",
    "    # \"captum_tracin\" : (CaptumTracInCPFast, captum_tracin_args),\n",
    "    # \"trak\" : (TRAK, trak_args),\n",
    "    \"representer\": (RepresenterPoints, representer_points_args),\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1b50409a",
   "metadata": {},
   "source": [
    "results = dict()\n",
    "for name, (cls, kwargs) in attributors.items():\n",
    "    results[name] = benchmark.evaluate(\n",
    "        explainer_cls=cls, expl_kwargs=kwargs, batch_size=8\n",
    "    )[\"score\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "34c5981b",
   "metadata": {},
   "source": [
    "The `results` dictionary contains the results of the evaluation. The keys are the names of the explainers and the values are dictionaries containing the results."
   ]
  },
  {
   "cell_type": "code",
   "id": "2bfdd374",
   "metadata": {},
   "source": [
    "results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1fe01197",
   "metadata": {},
   "source": [
    "# Assembling a benchmark from existing components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a1833a",
   "metadata": {},
   "source": [
    "You may want to handle the creation of each component differently, using different datasets, architectures, training paradigms or a higher/lower percentage of manipulated samples. We now showcase how to create and use a quanda `Benchmark` object to use these components in the evaluation process.\n",
    "\n",
    "To showcase different benchmarks, we will now switch to the `MislabelingDetection` benchmark. This benchmark evaluates the ability of data atttribution methods to identify mislabeled samples in the training dataset. This is done by training a model on a dataset which has a substantial amount of mislabeled samples. We then use the local data attribution methods to rank the training data. Original papers propose either using self-influence (i.e. the attribution of training samples on themselves) or some special methodology for each explainer (i.e. the global coefficients of the surrogate model in Representer Points). Quanda includes efficient implementation of self-influence or other strategies proposed in the original papers, whenever possible.\n",
    "\n",
    "This ranking is then used to go through the dataset to check mislabelings. Quanda computes the cumulative mislabeling detection curve and returns the AUC score with respect to this curve.\n",
    "\n",
    "Instead of creating the components from scratch, we will again download the benchmark and use collect the prepared components. We will then use the `MislabelingDetection.assemble` method to create the benchmark. Note that this is exactly what is happening when we are creating a benchmark using the `download` method."
   ]
  },
  {
   "cell_type": "code",
   "id": "3812734a",
   "metadata": {},
   "source": [
    "temp_benchmark = MislabelingDetection.download(\n",
    "    name=\"mnist_mislabeling_detection\",\n",
    "    cache_dir=cache_dir,\n",
    "    device=device,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eb9a2823",
   "metadata": {},
   "source": [
    "## Required Components\n",
    "\n",
    "In order to assemble a `MislabelingDetection` benchmark, we require the following components:\n",
    "- A base training dataset with correct labels.\n",
    "- A dictionary containing mislabeling information: integer keys are the indices of samples to change labels, and the values correspond to the new (wrong) labels that were used to train the model\n",
    "- A model trained on the mislabeled dataset\n",
    "- Number of classes in the dataset\n",
    "- Dataset transform that was used during training, applied to samples before feeding them to the model. If the base dataset already includes the transform, then we can just set this to `None`, which is the case in this tutorial. If the base dataset serves raw samples, then the `dataset_transform` parameter allows the usage of a transform.\n",
    "\n",
    "Let's collect these components from the downloaded benchmark. We then assemble the benchmark and evaluate the `RepresenterPoints` attributor with it. Note that the implementation depends on computing the self-influences of the whole training dataset. This procedure is fastest for the Representer Points attributor. Therefore, we use this explainer here."
   ]
  },
  {
   "cell_type": "code",
   "id": "d860e0bb",
   "metadata": {},
   "source": [
    "model = temp_benchmark.model\n",
    "base_dataset = temp_benchmark.base_dataset\n",
    "mislabeling_labels = temp_benchmark.mislabeling_labels\n",
    "checkpoints = temp_benchmark.checkpoints\n",
    "checkpoints_load_func = temp_benchmark.checkpoints_load_func\n",
    "dataset_transform = None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f6793457",
   "metadata": {},
   "source": [
    "## Assembling the benchmark and running the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "0ea26efd",
   "metadata": {},
   "source": [
    "benchmark = MislabelingDetection.assemble(\n",
    "    model=model,\n",
    "    base_dataset=base_dataset,\n",
    "    n_classes=10,\n",
    "    mislabeling_labels=mislabeling_labels,\n",
    "    checkpoints=checkpoints,\n",
    "    checkpoints_load_func=checkpoints_load_func,\n",
    "    dataset_transform=dataset_transform,\n",
    "    device=device,\n",
    ")\n",
    "representer_points_args = {\n",
    "    \"model_id\": \"mnist_mislabeling_detection\",\n",
    "    \"cache_dir\": os.path.join(cache_dir, \"representer_points\"),\n",
    "    \"batch_size\": 8,\n",
    "    \"features_layer\": \"model.relu_4\",\n",
    "    \"classifier_layer\": \"model.fc_3\",\n",
    "}\n",
    "benchmark.evaluate(\n",
    "    explainer_cls=RepresenterPoints,\n",
    "    expl_kwargs=representer_points_args,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bf670fbc",
   "metadata": {},
   "source": [
    "# Generating a Benchmark from Scratch\n",
    "\n",
    "We will now showcase how a benchmark can be created from only vanilla components. Quanda takes in all requires components and generates the benchmark, including dataset manipulations and model training, if applicable. Then the benchmark can be used to evaluate different attributors. This is done through the `Benchmark.generate` method.\n",
    "\n",
    "We will go through this use-case with the `SubclassDetection` benchmark which groups classes of the base dataset into superclasses. A model is trained to predict these super classes, and the original labelhighest attributed datapoint for each test sample is observed. The benchmark expects this to be the same as the true class of the test sample. \n",
    "\n",
    "As such, we only need to provide these components to generate the benchmark:\n",
    "\n",
    "- a model for the architecture\n",
    "- a trainer: either a subclass instance of quanda's `BaseTrainer` or a Lightning `Trainer` object. If the trainer is a Lightning trainer, the `model` has to be a Lightning module. We will use a Lightning trainer with a Lightning module.\n",
    "- a base dataset\n",
    "- an evaluation dataset to be used as the test set for generating the attributions to evaluate\n",
    "- a dataset transform. As in the case of `MislabelingDetection` explained above, the `dataset_transform` parameter can be `None` if the `base_dataset` and `eval_dataset` already include the required sample transformations. \n",
    "- the number of superclasses we want to generate the benchmark. \n",
    "\n",
    "Additionally, we can provide a dictionary which embodies a specific class grouping, or just use the default \"random\" value to randomly assign classes into superclasses, which is the approach we will take in this tutorial. Note that we will collect the base and evaluation datasets from the corresponding precomputed benchmark for simplicity and reproducibility. As such, these datasets will already include the transform required for sample normalization, which means we will supply `dataset_transform=None`.\n",
    "\n",
    "Please note that calling `SubclassDetection.generate` will initiate model training, therefore it will potentially take a long time."
   ]
  },
  {
   "cell_type": "code",
   "id": "d7028613",
   "metadata": {},
   "source": [
    "from quanda.benchmarks.resources import pl_modules\n",
    "import lightning as L\n",
    "\n",
    "num_groups = 2\n",
    "model = pl_modules[\"MnistModel\"](num_labels=num_groups, device=device)\n",
    "trainer = L.Trainer(max_epochs=5)\n",
    "dataset_transform = None\n",
    "\n",
    "# Collect base and evaluation datasets from a precomputed benchmark for simplicity, instead of creating the dataset objects from scratch\n",
    "base_dataset = temp_benchmark.base_dataset\n",
    "eval_dataset = temp_benchmark.eval_dataset\n",
    "\n",
    "\n",
    "benchmark = SubclassDetection.generate(\n",
    "    model=model,\n",
    "    trainer=trainer,\n",
    "    base_dataset=base_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_transform=dataset_transform,\n",
    "    n_classes=10,\n",
    "    n_groups=num_groups,\n",
    "    class_to_group=\"random\",\n",
    "    cache_dir=cache_dir,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0bb54ba0",
   "metadata": {},
   "source": [
    "Now that we have trained the model on the MNIST dataset with randomly grouped classes, we finalize this tutorial by evaluating the `CaptumSimilarity` attributor:"
   ]
  },
  {
   "cell_type": "code",
   "id": "e54ff456",
   "metadata": {},
   "source": [
    "benchmark.evaluate(\n",
    "    explainer_cls=CaptumSimilarity,\n",
    "    expl_kwargs={\n",
    "        \"model_id\": \"mnist_subclass_detection_tutorial\",\n",
    "        \"layers\": \"model.fc_2\",\n",
    "        \"cache_dir\": os.path.join(cache_dir, \"captum_similarity\"),\n",
    "        \"load_from_disk\": False,\n",
    "    },\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quanda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
