{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "![title](\"assets/demo/quanda_benchmarks_demo.png\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a59e422bb0b2c3d"
  },
  {
   "cell_type": "markdown",
   "id": "377c75d5",
   "metadata": {},
   "source": [
    "In this notebook, we will go through the different ways of creating an evaluation benchmark and comparing different attributors with it. First, we take the most straightforward route and go over the process of downloading a precomputed quanda benchmark for data attribution evaluation. This way, you can quickly write a quanda wrapper for your explainer and evaluate it against the existing explainers in the controlled setups we have prepared for you.\n",
    "\n",
    "Afterwards, we will go through the steps of assembling a benchmark from existing components. This option allows you to create your own controlled setup, and use quanda benchmarks for evaluation of different data attributors.\n",
    "\n",
    "Finally, we will summarize how to create your setup using quanda benchmarks, which includes managing datasets, training models and running evaluations.\n",
    "\n",
    "Throughout this tutorial, we will be using a LeNet model trained on the MNIST dataset.\n",
    "\n",
    "We first handle our include statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70692853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yolcu/miniconda3/envs/quanda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(),\"..\"))\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from quanda.benchmarks.downstream_eval import ShortcutDetection, MislabelingDetection, SubclassDetection\n",
    "from quanda.explainers.wrappers import (\n",
    "    TRAK,\n",
    "    CaptumArnoldi,\n",
    "    CaptumSimilarity,\n",
    "    CaptumTracInCPFast,\n",
    "    RepresenterPoints,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f3d5ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "to_img = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Normalize(mean=0.0, std=2.),\n",
    "    torchvision.transforms.Normalize(mean=-0.5, std=1.),\n",
    "    torchvision.transforms.ToPILImage(),\n",
    "    torchvision.transforms.Resize((224, 224)),])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1efc6e9",
   "metadata": {},
   "source": [
    "# Downloading Precomputed Benchmarks\n",
    "In this part of the tutorial, we will use the Shortcut Detection metric.\n",
    "\n",
    "We will use the benchmark corresponding to this metric to evaluate all data attributors currently included in quanda in terms of their ability to detect when the model is using a shortcut.\n",
    "\n",
    "We will download the precomputed MNIST benchmark. This includes an MNIST dataset which has shortcut features (an 8-by-8 white box on a specific location) on a subset of its samples from the class 0, and a model trained on this dataset. This model has learned to classify images with these features to the class 0, and we will measure the extent to which this is reflected in the attributions of different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1043f55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yolcu/Documents/Code/quanda/tutorials/../quanda/benchmarks/base.py:88: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(os.path.join(cache_dir, name + \".pth\"), map_location=device)\n"
     ]
    }
   ],
   "source": [
    "cache_dir = str(os.path.join(os.getcwd(), \"quanda_benchmark_tutorial_cache\"))\n",
    "device=\"cuda\"\n",
    "benchmark = ShortcutDetection.download(\n",
    "    name=\"mnist_shortcut_detection\",\n",
    "    cache_dir=cache_dir,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe371b0c",
   "metadata": {},
   "source": [
    "The benchmark object contains all information about the controlled evaluation setup. Let's see some samples with the shortcut features, using benchmark.feature_dataset and benchmark.shortcut_indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e516d03e70a7710f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=224x224>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortcut_img = benchmark.shortcut_dataset[benchmark.shortcut_indices[15]][0]\n",
    "tensor_img=torch.concat([shortcut_img,shortcut_img,shortcut_img],dim=0)\n",
    "img=to_img(tensor_img)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2f4e896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9986)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "for i in benchmark.shortcut_indices:\n",
    "    x,y = benchmark.shortcut_dataset[i]\n",
    "    x=x.to(device)\n",
    "    benchmark.model(x[None])\n",
    "    predictions.append(benchmark.model(x[None]).argmax().item())\n",
    "predictions=torch.tensor(predictions)\n",
    "shortcut_rate=torch.mean((predictions==benchmark.shortcut_cls)*1.0)\n",
    "shortcut_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea369608",
   "metadata": {},
   "source": [
    "## Prepare initialization parameters for TDA methods\n",
    "\n",
    "We now prepare the initialization parameters of attributors: hyperparameters, and components from the benchmark as needed. Note that we do not provide the model and dataset to use for attribution, since those components will be supplied by the benchmark objects, while initializing the attributor during evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc91dc4",
   "metadata": {},
   "source": [
    "### Similarity Influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c59e012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "captum_similarity_args = {\n",
    "    \"model_id\": \"mnist_shortcut_detection_tutorial\",\n",
    "    \"layers\": \"model.fc_2\",\n",
    "    \"cache_dir\": os.path.join(cache_dir, \"captum_similarity\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ef1e78",
   "metadata": {},
   "source": [
    "### Arnoldi Influence Functions\n",
    "\n",
    "Notice that the trained checkpoints have been saved to the `cache_dir` while downloading the benchmark. We can reach the paths of these checkpoints with `benchmark.checkpoint_paths`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9439b6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian_num_samples=500 # number of samples to use for hessian estimation\n",
    "hessian_ds=torch.utils.data.Subset(benchmark.shortcut_dataset, torch.randint(0, len(benchmark.shortcut_dataset), (hessian_num_samples,)))\n",
    "\n",
    "captum_influence_args = {\n",
    "        \"checkpoint\": benchmark.checkpoint_paths[-1],\n",
    "        \"layers\": [\"model.fc_3\"],\n",
    "        \"batch_size\": 8,\n",
    "        \"hessian_dataset\": hessian_ds,\n",
    "        \"projection_dim\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fd5356",
   "metadata": {},
   "source": [
    "### TracInCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d3963d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "captum_tracin_args = {\n",
    "    \"final_fc_layer\": \"model.fc_3\",\n",
    "    \"loss_fn\": torch.nn.CrossEntropyLoss(reduction=\"mean\"),\n",
    "    \"checkpoints\": benchmark.checkpoint_paths,\n",
    "    \"batch_size\": 8,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe480a29",
   "metadata": {},
   "source": [
    "### TRAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4430e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trak_args = {\n",
    "    \"model_id\": \"mnist_shortcut_detection\",\n",
    "    \"cache_dir\": os.path.join(cache_dir, \"trak\"),\n",
    "    \"batch_size\": 8,\n",
    "    \"proj_dim\": 2048,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea6de8e",
   "metadata": {},
   "source": [
    "### Representer Points Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5287f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "representer_points_args = {\n",
    "    \"model_id\": \"mnist_shortcut_detection\",\n",
    "    \"cache_dir\": os.path.join(cache_dir, \"representer_points\"),\n",
    "    \"batch_size\": 8,\n",
    "    \"features_layer\": \"model.relu_4\",\n",
    "    \"classifier_layer\": \"model.fc_3\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417f773a",
   "metadata": {},
   "source": [
    "## Run the benchmark evaluation on the attributors\n",
    "Note that some attributors take a long time to initialize or compute attributions. For a proof of concept, we recommend using `CaptumSimilarity` or `RepresenterPoints`, or lowering the parameter values given above (i.e. using low `proj_dim` for TRAK or a low Hessian dataset size for Arnoldi Influence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d020f5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributors={\n",
    "    # please comment out the explainers you are not interested in\n",
    "    \"captum_similarity\": (CaptumSimilarity, captum_similarity_args),\n",
    "    #\"captum_arnoldi\" : (CaptumArnoldi, captum_influence_args),\n",
    "    #\"captum_tracin\" : (CaptumTracInCPFast, captum_tracin_args),\n",
    "    #\"trak\" : (TRAK, trak_args),\n",
    "    \"representer\" : (RepresenterPoints, representer_points_args),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b50409a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 20:17:53,321 - quanda.explainers.wrappers.captum_influence - INFO - Initializing Captum SimilarityInfluence explainer...\n",
      "Metric evaluation, batch 1/16:   0%|          | 0/16 [00:00<?, ?it/s]/home/yolcu/miniconda3/envs/quanda/lib/python3.11/site-packages/torch/__init__.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)\n",
      "  _C._set_default_tensor_type(t)\n",
      "/home/yolcu/Documents/Code/quanda/tutorials/../quanda/utils/common.py:233: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return original_load(f, *args, **kwargs)\n",
      "Metric evaluation, batch 16/16: 100%|██████████| 16/16 [17:40<00:00, 66.28s/it]\n",
      "2024-09-30 20:36:12,570 - quanda.explainers.wrappers.representer_points - INFO - Initializing Representer Point Selection explainer...\n",
      "/home/yolcu/Documents/Code/quanda/tutorials/../quanda/explainers/wrappers/representer_points.py:118: UserWarning: This method is only a good idea for small datasets and small architectures. Otherwise, this will consume a lot of memory.\n",
      "  warnings.warn(\n",
      "Metric evaluation, batch 16/16: 100%|██████████| 16/16 [00:00<00:00, 130.08it/s]\n"
     ]
    }
   ],
   "source": [
    "results=dict()\n",
    "for name, (cls, kwargs) in attributors.items():\n",
    "    results[name] = benchmark.evaluate(\n",
    "        explainer_cls=cls,\n",
    "        expl_kwargs=kwargs,\n",
    "        batch_size=8\n",
    "    )[\"score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c5981b",
   "metadata": {},
   "source": [
    "The `results` dictionary contains the results of the evaluation. The keys are the names of the explainers and the values are dictionaries containing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bfdd374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'captum_similarity': 0.41747230291366577, 'representer': 0.4443182349205017}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe01197",
   "metadata": {},
   "source": [
    "# Assembling a benchmark from existing components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a1833a",
   "metadata": {},
   "source": [
    "You may want to handle the creation of each component differently, using different datasets, architectures, training paradigms or a higher/lower percentage of manipulated samples. We now showcase how to create and use a quanda `Benchmark` object to use these components in the evaluation process.\n",
    "\n",
    "To showcase different benchmarks, we will now switch to the `MislabelingDetection` benchmark. This benchmark evaluates the ability of data atttribution methods to identify mislabeled samples in the training dataset. This is done by training a model on a dataset which has a substantial amount of mislabeled samples. We then use the local data attribution methods to rank the training data. Original papers propose either using self-influence (i.e. the attribution of training samples on themselves) or some special methodology for each explainer (i.e. the global coefficients of the surrogate model in Representer Points). Quanda includes efficient implementation of self-influence or other strategies proposed in the original papers, whenever possible.\n",
    "\n",
    "This ranking is then used to go through the dataset to check mislabelings. Quanda computes the cumulative mislabeling detection curve and returns the AUC score with respect to this curve.\n",
    "\n",
    "Instead of creating the components from scratch, we will again download the benchmark and use collect the prepared components. We will then use the `MislabelingDetection.assemble` method to create the benchmark. Note that this is exactly what is happening when we are creating a benchmark using the `download` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3812734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_benchmark = MislabelingDetection.download(\n",
    "    name=\"mnist_mislabeling_detection\",\n",
    "    cache_dir=cache_dir,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a2823",
   "metadata": {},
   "source": [
    "## Required Components\n",
    "\n",
    "In order to assemble a `MislabelingDetection` benchmark, we require the following components:\n",
    "- A base training dataset with correct labels.\n",
    "- A dictionary containing mislabeling information: integer keys are the indices of samples to change labels, and the values correspond to the new (wrong) labels that were used to train the model\n",
    "- A model trained on the mislabeled dataset\n",
    "- Number of classes in the dataset\n",
    "- Dataset transform that was used during training, applied to samples before feeding them to the model. If the base dataset already includes the transform, then we can just set this to `None`, which is the case in this tutorial. If the base dataset serves raw samples, then the `dataset_transform` parameter allows the usage of a transform.\n",
    "\n",
    "Let's collect these components from the downloaded benchmark. We then assemble the benchmark and evaluate the `RepresenterPoints` attributor with it. Note that the implementation depends on computing the self-influences of the whole training dataset. This procedure is fastest for the Representer Points attributor. Therefore, we use this explainer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d860e0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=temp_benchmark.model\n",
    "base_dataset=temp_benchmark.base_dataset\n",
    "mislabeling_labels=temp_benchmark.mislabeling_labels\n",
    "dataset_transform=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6793457",
   "metadata": {},
   "source": [
    "## Assembling the benchmark and running the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ea26efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 20:37:18,012 - quanda.explainers.wrappers.representer_points - INFO - Initializing Representer Point Selection explainer...\n",
      "2024-09-30 20:37:49,425 - quanda.explainers.wrappers.representer_points - INFO - Initializing Representer Point Selection explainer...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.6750942468643188,\n",
       " 'success_arr': tensor([False, False, False,  ..., False, False, False]),\n",
       " 'curve': tensor([0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.5907e-05, 5.5907e-05,\n",
       "         5.5907e-05])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark=MislabelingDetection.assemble(\n",
    "    model=model,\n",
    "    base_dataset=base_dataset,\n",
    "    n_classes=10,\n",
    "    mislabeling_labels=mislabeling_labels,\n",
    "    dataset_transform=dataset_transform,\n",
    "    device=device,\n",
    ")\n",
    "representer_points_args = {\n",
    "    \"model_id\": \"mnist_mislabeling_detection\",\n",
    "    \"cache_dir\": os.path.join(cache_dir, \"representer_points\"),\n",
    "    \"batch_size\": 8,\n",
    "    \"features_layer\": \"model.relu_4\",\n",
    "    \"classifier_layer\": \"model.fc_3\",\n",
    "}\n",
    "benchmark.evaluate(\n",
    "    explainer_cls=RepresenterPoints,\n",
    "    expl_kwargs=representer_points_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf670fbc",
   "metadata": {},
   "source": [
    "# Generating a Benchmark from Scratch\n",
    "\n",
    "We will now showcase how a benchmark can be created from only vanilla components. Quanda takes in all requires components and generates the benchmark, including dataset manipulations and model training, if applicable. Then the benchmark can be used to evaluate different attributors. This is done through the `Benchmark.generate` method.\n",
    "\n",
    "We will go through this use-case with the `SubclassDetection` benchmark which groups classes of the base dataset into superclasses. A model is trained to predict these super classes, and the original labelhighest attributed datapoint for each test sample is observed. The benchmark expects this to be the same as the true class of the test sample. \n",
    "\n",
    "As such, we only need to provide these components to generate the benchmark:\n",
    "\n",
    "- a model for the architecture\n",
    "- a trainer: either a subclass instance of quanda's `BaseTrainer` or a Lightning `Trainer` object. If the trainer is a Lightning trainer, the `model` has to be a Lightning module. We will use a Lightning trainer with a Lightning module.\n",
    "- a base dataset\n",
    "- an evaluation dataset to be used as the test set for generating the attributions to evaluate\n",
    "- a dataset transform. As in the case of `MislabelingDetection` explained above, the `dataset_transform` parameter can be `None` if the `base_dataset` and `eval_dataset` already include the required sample transformations. \n",
    "- the number of superclasses we want to generate the benchmark. \n",
    "\n",
    "Additionally, we can provide a dictionary which embodies a specific class grouping, or just use the default \"random\" value to randomly assign classes into superclasses, which is the approach we will take in this tutorial. Note that we will collect the base and evaluation datasets from the corresponding precomputed benchmark for simplicity and reproducibility. As such, these datasets will already include the transform required for sample normalization, which means we will supply `dataset_transform=None`.\n",
    "\n",
    "Please note that calling `SubclassDetection.generate` will initiate model training, therefore it will potentially take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7028613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/yolcu/miniconda3/envs/quanda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "2024-09-30 20:38:21,231 - quanda.benchmarks.downstream_eval.subclass_detection - INFO - Generating Subclass Detection benchmark components based on passed arguments...\n",
      "/home/yolcu/miniconda3/envs/quanda/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | LeNet5           | 43.7 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "43.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "43.7 K    Total params\n",
      "0.175     Total estimated model params size (MB)\n",
      "/home/yolcu/miniconda3/envs/quanda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 7500/7500 [01:31<00:00, 81.78it/s, v_num=7, train_loss_step=0.000189, train_loss_epoch=0.00589]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 7500/7500 [01:31<00:00, 81.77it/s, v_num=7, train_loss_step=0.000189, train_loss_epoch=0.00589]\n"
     ]
    }
   ],
   "source": [
    "from quanda.benchmarks.resources import pl_modules\n",
    "import lightning as L\n",
    "\n",
    "num_groups=2\n",
    "model=pl_modules[\"MnistModel\"](num_labels=num_groups, device=device)\n",
    "trainer = L.Trainer(max_epochs=10)\n",
    "dataset_transform = None\n",
    "\n",
    "#Collect base and evaluation datasets from a precomputed benchmark for simplicity, instead of creating the dataset objects from scratch\n",
    "base_dataset = temp_benchmark.base_dataset\n",
    "eval_dataset = temp_benchmark.eval_dataset\n",
    "\n",
    "\n",
    "benchmark=SubclassDetection.generate(\n",
    "    model=model,\n",
    "    trainer=trainer,\n",
    "    base_dataset=base_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_transform=dataset_transform,\n",
    "    n_classes=10,\n",
    "    n_groups=num_groups,\n",
    "    class_to_group=\"random\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb54ba0",
   "metadata": {},
   "source": [
    "Now that we have trained the model on the MNIST dataset with randomly grouped classes, we finalize this tutorial by evaluating the `CaptumSimilarity` attributor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e54ff456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 21:15:13,556 - quanda.explainers.wrappers.captum_influence - INFO - Initializing Captum SimilarityInfluence explainer...\n",
      "Metric evaluation, batch 1/16:   0%|          | 0/16 [00:00<?, ?it/s]/home/yolcu/miniconda3/envs/quanda/lib/python3.11/site-packages/torch/__init__.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)\n",
      "  _C._set_default_tensor_type(t)\n",
      "/home/yolcu/Documents/Code/quanda/tutorials/../quanda/utils/common.py:233: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return original_load(f, *args, **kwargs)\n",
      "Metric evaluation, batch 16/16: 100%|██████████| 16/16 [14:52<00:00, 55.75s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.3054291903972626}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark.evaluate(\n",
    "    explainer_cls=CaptumSimilarity,\n",
    "    expl_kwargs={\n",
    "    \"model_id\": \"mnist_subclass_detection_tutorial\",\n",
    "    \"layers\": \"model.fc_2\",\n",
    "    \"cache_dir\": os.path.join(cache_dir, \"captum_similarity\"),\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6551bf2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
