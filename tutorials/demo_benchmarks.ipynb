{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![quanda_benchmarks_demo.png](attachment:quanda_benchmarks_demo.png)"
   ],
   "id": "7a5012fd10f11fe2"
  },
  {
   "cell_type": "markdown",
   "id": "dc8bc781",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this notebook, we will go through the different ways of creating an evaluation benchmark and comparing different attributors with it. First, we take the most straightforward route and go over the process of downloading a precomputed quanda benchmark for data attribution evaluation. This way, you can quickly write a quanda wrapper for your explainer and evaluate it against the existing explainers in the controlled setups we have prepared for you.\n",
    "\n",
    "Afterwards, we will go through the steps of assembling a benchmark from existing components. This option allows you to create your own controlled setup, and use quanda benchmarks for evaluation of different data attributors.\n",
    "\n",
    "Finally, we will summarize how to create your setup using quanda benchmarks, which includes managing datasets, training models and running evaluations.\n",
    "\n",
    "Throughout this tutorial, we will be using a LeNet model trained on the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c75d5",
   "metadata": {},
   "source": [
    "We first handle our include statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70692853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from quanda.benchmarks.downstream_eval import ShortcutDetection, MislabelingDetection, SubclassDetection\n",
    "from quanda.explainers.wrappers import (\n",
    "    TRAK,\n",
    "    CaptumArnoldi,\n",
    "    CaptumSimilarity,\n",
    "    CaptumTracInCPFast,\n",
    "    RepresenterPoints,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "to_img = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Normalize(mean=0.0, std=2.),\n",
    "    torchvision.transforms.Normalize(mean=-0.5, std=1.),\n",
    "    torchvision.transforms.ToPILImage(),\n",
    "    torchvision.transforms.Resize((224, 224)),])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8186909fb4cc6da"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Downloading Precomputed Benchmarks\n",
    "In this part of the tutorial, we will use the Shortcut Detection metric.\n",
    "\n",
    "We will use the benchmark corresponding to this metric to evaluate all data attributors currently included in quanda in terms of their ability to detect when the model is using a shortcut.\n",
    "\n",
    "We will download the precomputed MNIST benchmark. This includes an MNIST dataset which has shortcut features (an 8-by-8 white box on a specific location) on a subset of its samples from the class 0, and a model trained on this dataset. This model has learned to classify images with these features to the class 0, and we will measure the extent to which this is reflected in the attributions of different methods."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b0793953e53bae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cache_dir = str(os.path.join(os.getcwd(), \"quanda_benchmark_tutorial_cache\"))\n",
    "device=\"cpu\"\n",
    "benchmark = ShortcutDetection.download(\n",
    "    name=\"mnist_shortcut_detection\",\n",
    "    cache_dir=cache_dir,\n",
    "    device=device,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d25c6d4f059a291"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The benchmark object contains all information about the controlled evaluation setup. Let's see some samples with the shortcut features, using benchmark.feature_dataset and benchmark.shortcut_indices. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c13bc08c395817ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shortcut_img = benchmark.shortcut_dataset[benchmark.shortcut_indices[15]][0]\n",
    "tensor_img=torch.concat([shortcut_img,shortcut_img,shortcut_img],dim=0)\n",
    "img=to_img(tensor_img)\n",
    "img"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f24da593bbd3321"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i in benchmark.shortcut_indices:\n",
    "    x,y = benchmark.shortcut_dataset[i]\n",
    "    x=x.to(device)\n",
    "    benchmark.model(x[None])\n",
    "    predictions.append(benchmark.model(x[None]).argmax().item())\n",
    "predictions=torch.tensor(predictions)\n",
    "shortcut_rate=torch.mean((predictions==benchmark.shortcut_cls)*1.0)\n",
    "shortcut_rate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71913a8cad000ed2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare initialization parameters for TDA methods\n",
    "\n",
    "We now prepare the initialization parameters of attributors: hyperparameters, and components from the benchmark as needed. Note that we do not provide the model and dataset to use for attribution, since those components will be supplied by the benchmark objects, while initializing the attributor during evaluation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a67ca0e50f3c963"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Similarity Influence"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc64beac8b710ae7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "captum_similarity_args = {\n",
    "    \"model_id\": \"mnist_shortcut_detection_tutorial\",\n",
    "    \"layers\": \"model.fc_2\",\n",
    "    \"cache_dir\": os.path.join(cache_dir, \"captum_similarity\"),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bef8a660ed80061f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Arnoldi Influence Functions\n",
    "\n",
    "Notice that the trained checkpoints have been saved to the `cache_dir` while downloading the benchmark. We can reach the paths of these checkpoints with `benchmark.checkpoint_paths`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1480f27afb02646d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hessian_num_samples=500 # number of samples to use for hessian estimation\n",
    "hessian_ds=torch.utils.data.Subset(benchmark.shortcut_dataset, torch.randint(0, len(benchmark.shortcut_dataset), (hessian_num_samples,)))\n",
    "\n",
    "captum_influence_args = {\n",
    "        \"checkpoint\": benchmark.checkpoint_paths[-1],\n",
    "        \"layers\": [\"model.fc_3\"],\n",
    "        \"batch_size\": 8,\n",
    "        \"hessian_dataset\": hessian_ds,\n",
    "        \"projection_dim\": 5,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f10e52f2cf0f664c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TracInCP"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b56d48a7733f4f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "captum_tracin_args = {\n",
    "    \"final_fc_layer\": \"model.fc_3\",\n",
    "    \"loss_fn\": torch.nn.CrossEntropyLoss(reduction=\"mean\"),\n",
    "    \"checkpoints\": benchmark.checkpoint_paths,\n",
    "    \"batch_size\": 8,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e0da7f696d5694d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TRAK"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "764a399309e17755"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trak_args = {\n",
    "    \"model_id\": \"mnist_shortcut_detection\",\n",
    "    \"cache_dir\": os.path.join(cache_dir, \"trak\"),\n",
    "    \"batch_size\": 8,\n",
    "    \"proj_dim\": 5,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62223a7cd9f9e804"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Representer Points Selection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1e54c3af53b3a81"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "representer_points_args = {\n",
    "    \"model_id\": \"mnist_shortcut_detection\",\n",
    "    \"cache_dir\": os.path.join(cache_dir, \"representer_points\"),\n",
    "    \"batch_size\": 8,\n",
    "    \"features_layer\": \"model.relu_4\",\n",
    "    \"classifier_layer\": \"model.fc_3\",\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "378e44d0c3272547"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run the benchmark evaluation on the attributors\n",
    "Note that some attributors take a long time to initialize or compute attributions. For a proof of concept, we recommend using `CaptumSimilarity` or `RepresenterPoints`, or lowering the parameter values given above (i.e. using low `proj_dim` for TRAK or a low Hessian dataset size for Arnoldi Influence)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c24ea7f1ec5c4f37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "attributors={\n",
    "    # please comment out the explainers you are not interested in\n",
    "    \"captum_similarity\": (CaptumSimilarity, captum_similarity_args),\n",
    "    #\"captum_arnoldi\" : (CaptumArnoldi, captum_influence_args),\n",
    "    #\"captum_tracin\" : (CaptumTracInCPFast, captum_tracin_args),\n",
    "    #\"trak\" : (TRAK, trak_args),\n",
    "    #\"representer\" : (RepresenterPoints, representer_points_args),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2205cefaa3a68c1e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results=dict()\n",
    "for name, (cls, kwargs) in attributors.items():\n",
    "    results[name] = benchmark.evaluate(\n",
    "        explainer_cls=cls,\n",
    "        expl_kwargs=kwargs,\n",
    "        batch_size=128\n",
    "    )[\"score\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee0156cdc92ea4d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `results` dictionary contains the results of the evaluation. The keys are the names of the explainers and the values are dictionaries containing the results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69596304035147a9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b256a563f34c316"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Assembling a benchmark from existing components"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d93d293b6d98a5f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You may want to handle the creation of each component differently, using different datasets, architectures, training paradigms or a higher/lower percentage of manipulated samples. We now showcase how to create and use a quanda `Benchmark` object to use these components in the evaluation process.\n",
    "\n",
    "To showcase different benchmarks, we will now switch to the `MislabelingDetection` benchmark. This benchmark evaluates the ability of data atttribution methods to identify mislabeled samples in the training dataset. This is done by training a model on a dataset which has a substantial amount of mislabeled samples. We then use the local data attribution methods to rank the training data. Original papers propose either using self-influence (i.e. the attribution of training samples on themselves) or some special methodology for each explainer (i.e. the global coefficients of the surrogate model in Representer Points). Quanda includes efficient implementation of self-influence or other strategies proposed in the original papers, whenever possible.\n",
    "\n",
    "This ranking is then used to go through the dataset to check mislabelings. Quanda computes the cumulative mislabeling detection curve and returns the AUC score with respect to this curve.\n",
    "\n",
    "Instead of creating the components from scratch, we will again download the benchmark and use collect the prepared components. We will then use the `MislabelingDetection.assemble` method to create the benchmark. Note that this is exactly what is happening when we are creating a benchmark using the `download` method."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6479f568b8d8c40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_benchmark = MislabelingDetection.download(\n",
    "    name=\"mnist_mislabeling_detection\",\n",
    "    cache_dir=cache_dir,\n",
    "    device=device,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a885e3263065a0a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Required Components\n",
    "\n",
    "In order to assemble a `MislabelingDetection` benchmark, we require the following components:\n",
    "- A base training dataset with correct labels\n",
    "- A dictionary containing mislabeling information: integer keys are the indices of samples to change labels, and the values correspond to the new (wrong) labels that were used to train the model\n",
    "- A model trained on the mislabeled dataset\n",
    "- Number of classes in the dataset\n",
    "- Dataset transform that was used during training, applied to samples before feeding them to the model\n",
    "\n",
    "Let's collect these components from the downloaded benchmark. We then assemble the benchmark and evaluate the `CaptumSimilarity` attributor with it."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "287ec6c750e92cea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model=temp_benchmark.model\n",
    "base_dataset=temp_benchmark.base_dataset\n",
    "mislabeling_labels=temp_benchmark.mislabeling_labels\n",
    "dataset_transform=temp_benchmark.dataset_transform\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc4b7fd48c86ec1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "benchmark=MislabelingDetection.assemble(\n",
    "    model=model,\n",
    "    base_dataset=base_dataset,\n",
    "    n_classes=10,\n",
    "    mislabeling_labels=mislabeling_labels,\n",
    "    dataset_transform=dataset_transform,\n",
    "    device=device,\n",
    ")\n",
    "benchmark.evaluate(\n",
    "    CaptumSimilarity,\n",
    "    captum_similarity_args\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec73fd4cf404ab7d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7028613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
