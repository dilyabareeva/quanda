# START1
from torch.utils.data import DataLoader
from tqdm import tqdm

from quanda.explainers.wrappers import CaptumSimilarity
from quanda.metrics.heuristics import ModelRandomizationMetric
# END1

# START2
DEVICE = "cpu"
model.to(DEVICE)
explainer_kwargs = {
    "layers": "avgpool",
    "model_id": "default_model_id",
    "cache_dir": "./cache"
}
explainer = CaptumSimilarity(
    model=model,
    train_dataset=train_set,
    **explainer_kwargs
)
# END2

# START3
explainer_kwargs = {
    "layers": "avgpool",
    "model_id": "randomized_model_id",
    "cache_dir": "./cache"
}
model_rand = ModelRandomizationMetric(
    model=model,
    train_dataset=train_set,
    explainer_cls=CaptumSimilarity,
    expl_kwargs=explainer_kwargs,
    correlation_fn="spearman",
    seed=42,
)
# END3

# START4
test_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False)
for test_tensor, _ in tqdm(test_loader):
    test_tensor = test_tensor.to(DEVICE)
    target = model(test_tensor).argmax(dim=-1)
    tda = explainer.explain(
        test_tensor=test_tensor,
        targets=target
    )
    model_rand.update(test_data=test_tensor, explanations=tda, explanation_targets=target)

print("Randomization metric output:", model_rand.compute())
# END4

# START5
from quanda.explainers.wrappers import CaptumSimilarity
from quanda.benchmarks.downstream_eval import SubclassDetection
# END5

# START6
DEVICE = "cpu"
model.to(DEVICE)

explainer_kwargs = {
    "layers": "avgpool",
    "model_id": "default_model_id",
    "cache_dir": "./cache"
}
# END6

# START7
subclass_detect = SubclassDetection.download(
    name="mnist_subclass_detection",
    cache_dir=cache_dir,
    device="cpu",
)
score = subclass_detect.evaluate(
    explainer_cls=CaptumSimilarity,
    expl_kwargs=explain_fn_kwargs,
    batch_size=batch_size,
)["score"]
print(f"Subclass Detection Score: {score}")
# END7

# START8
from quanda.explainers.wrappers import CaptumSimilarity
from quanda.benchmarks.ground_truth import TopKCardinality
# END8

# START9
DEVICE = "cpu"
model.to(DEVICE)

explainer_kwargs = {
    "layers": "avgpool",
    "model_id": "default_model_id",
    "cache_dir": "./cache"
}
# END9

# START10
topk_cardinality = TopKCardinality.assemble(
    model=model,
    train_dataset=train_set,
    eval_dataset=eval_set,
)
score = topk_cardinality.evaluate(
    explainer_cls=CaptumSimilarity,
    expl_kwargs=explain_fn_kwargs,
    batch_size=batch_size,
)["score"]
print(f"Top K Cardinality Score: {score}")
# END10

# START11
import torch

from quanda.explainers.wrappers import CaptumSimilarity
from quanda.benchmarks.downstream_eval import MislabelingDetection
# END11

# START12
DEVICE = "cpu"
model.to(DEVICE)

explainer_kwargs = {
    "layers": "avgpool",
    "model_id": "default_model_id",
    "cache_dir": "./cache"
}
# END12

# START13
trainer = Trainer(
    max_epochs=100,
    optimizer=torch.optim.SGD,
    lr=0.01,
    criterion=torch.nn.CrossEntropyLoss(),
)
# END13

# START14
mislabeling_detection = MislabelingDetection.generate(
    model=model,
    base_dataset=train_set,
    n_classes=n_classes,
    trainer=trainer,
)
score = mislabeling_detection.evaluate(
    explainer_cls=CaptumSimilarity,
    expl_kwargs=explain_fn_kwargs,
    batch_size=batch_size,
)["score"]
print(f"Mislabeling Detection Score: {score}")
# END14

# START15
from quanda.explainers.base import Explainer

class CustomExplainer(Explainer):
    def __init__(self, model, train_dataset, **kwargs):
        super().__init__(model, train_dataset, **kwargs)
        # Initialize your explainer here
# END15

# START16
def explain(
    self,
    test_tensor: torch.Tensor,
    targets: Union[List[int], torch.Tensor]
) -> torch.Tensor:
    # Compute your influence scores here
    return influence_scores
# END16

# START17
def self_influence(self, batch_size: int = 1) -> torch.Tensor:
    # Compute your self-influence scores here
    return self_influence_scores
# END17