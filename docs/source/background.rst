What is Training Data Attribution?
==========

The interpretability of neural network decisions is an active area of research which has seen a variety of approaches over time. Most of the initial focus was on feature attribution methods, which highlight features in the input space that are responsible for a specific prediction (`Simonyan et al., 2014 <https://arxiv.org/abs/1312.6034>`_; `Bach et al., 2015 <https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140>`_; `Lundberg and Lee, 2017 <https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf>`_). These methods were often criticized for being unreliable and difficult to understand (`Adebayo et al., 2018 <https://proceedings.neurips.cc/paper_files/paper/2018/file/294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf>`_; `Ghorbani et al., 2019 <https://ojs.aaai.org/index.php/AAAI/article/view/4252>`_). In response, researchers explored new directions, such as concept-based (`Poeta et al., 2023 <https://arxiv.org/abs/2312.12936>`_) and mechanistic interpretability (`Bereska and Gavves <https://openreview.net/forum?id=ePUVetPKu6>`_) methods. Recently, **Training Data Attribution** (TDA) has gained attention as a promising approach for enhancing the interpretability of neural networks.

TDA methods attribute model output on a specific test sample to the training dataset that it was trained on. As such, they reveal the training datapoints responsible for the model's decisions. Tracing model decisions back to the training data, TDA methods enable practitioners to understand the model's behavior and identify potential issues in the training setup, such as biases in the dataset. Different approaches have been proposed for this problem. While some methods focus on estimating the counterfactual effect of removing datapoints from the training set and retraining the model (`Koh and Liang, 2017 <https://proceedings.mlr.press/v70/koh17a.html>`_; `Park et al., 2023 <https://proceedings.mlr.press/v202/park23c.html>`_; `Bae et al., 2024 <https://arxiv.org/abs/2405.12186>`_), other methods achieve the attribution by tracking the contributions of training points to the loss reduction throughout training (`Pruthi et al., 2020 <https://proceedings.neurips.cc/paper/2020/hash/e6385d39ec9394f2f3a354d9d2b88eec-Abstract.html>`_), using interpretable surrogate models (`Yeh et al., 2018 <https://proceedings.neurips.cc/paper/2018/hash/8a7129b8f3edd95b7d969dfc2c8e9d9d-Abstract.html>`_) or finding training samples that are deemed similar to the test sample by the model (`Caruana et al., 1999 <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232607/>`_; `Hanawa et al., 2021 <https://openreview.net/forum?id=9uvhpyQwzM_>`_). In addition to model understanding, TDA has been used in a variety of applications such as debugging model behavior (`Koh and Liang, 2017 <https://proceedings.mlr.press/v70/koh17a.html>`_; `Yeh et al., 2018 <https://proceedings.neurips.cc/paper/2018/hash/8a7129b8f3edd95b7d969dfc2c8e9d9d-Abstract.html>`_; `K and SÃ¸gaard, 2021 <https://arxiv.org/abs/2111.04683>`_; `Guo et al., 2021 <https://aclanthology.org/2021.emnlp-main.808>`_), data summarization (`Khanna et al., 2019 <https://proceedings.mlr.press/v89/khanna19a.html>`_; `Marion et al., 2023 <https://openreview.net/forum?id=XUIYn3jo5T>`_; `Yang et al., 2023 <https://openreview.net/forum?id=4wZiAXD29TQ>`_), dataset selection (`Engstrom et al., 2024 <https://openreview.net/forum?id=GC8HkKeH8s>`_; `Chhabra et al., 2024 <https://openreview.net/forum?id=HE9eUQlAvo>`_), fact tracing (`Akyurek et al., 2022 <https://aclanthology.org/2022.findings-emnlp.180>`_) and machine unlearning (`Warnecke
et al., 2023 <https://arxiv.org/abs/2108.11577>`_).