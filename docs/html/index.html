<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Welcome to quanda’s documentation! &mdash; quanda 01.07.2024 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=cea27b2f"></script>
        <script src="_static/doctools.js?v=9a2dae69"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Getting Started" href="getting_started.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="#" >
            
              <img src="_static/quanda_white.png" class="logo" alt="Logo" style="width: 70%; height: auto;"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting Started</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="docs_api/modules.html">quanda</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">quanda</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Welcome to quanda’s documentation!</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="welcome-to-quanda-s-documentation">
<h1>Welcome to quanda’s documentation!<a class="headerlink" href="#welcome-to-quanda-s-documentation" title="Link to this heading"></a></h1>
<p><strong>quanda</strong> is a toolkit for <strong>quan</strong>titative evaluation of <strong>d</strong>ata <strong>a</strong>ttribution methods in <strong>PyTorch</strong>.</p>
<p><strong>Training data attribution</strong> (TDA) methods attribute model output on a specific test sample to the training dataset that it was trained on. They reveal the training datapoints responsible for the model’s decisions. Existing methods achieve this by estimating the counterfactual effect of removing datapoints from the training set (<a class="reference external" href="https://proceedings.mlr.press/v70/koh17a.html">Koh and Liang, 2017</a>; <a class="reference external" href="https://proceedings.mlr.press/v202/park23c.html">Park et al., 2023</a>; <a class="reference external" href="https://arxiv.org/abs/2405.12186">Bae et al., 2024</a>) tracking the contributions of training points to the loss reduction throughout training (<a class="reference external" href="https://proceedings.neurips.cc/paper/2020/hash/e6385d39ec9394f2f3a354d9d2b88eec-Abstract.html">Pruthi et al., 2020</a>), using interpretable surrogate models (<a class="reference external" href="https://proceedings.neurips.cc/paper/2018/hash/8a7129b8f3edd95b7d969dfc2c8e9d9d-Abstract.html">Yeh et al., 2018</a>) or finding training samples that are deemed similar to the test sample by the model (<a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2232607/">Caruana et. al, 1999</a>; <a class="reference external" href="https://openreview.net/forum?id=9uvhpyQwzM_">Hanawa et. al, 2021</a>). In addition to model understanding, TDA has been used in a variety of applications such as debugging model behavior (<a class="reference external" href="https://proceedings.mlr.press/v70/koh17a.html">Koh and Liang, 2017</a>; <a class="reference external" href="https://proceedings.neurips.cc/paper/2018/hash/8a7129b8f3edd95b7d969dfc2c8e9d9d-Abstract.html">Yeh et al., 2018</a>; <a class="reference external" href="https://arxiv.org/abs/2111.04683">K and Søgaard, 2021</a>; <a class="reference external" href="https://aclanthology.org/2021.emnlp-main.808">Guo et al., 2021</a>), data summarization (<a class="reference external" href="https://proceedings.mlr.press/v89/khanna19a.html">Khanna et al., 2019</a>; <a class="reference external" href="https://openreview.net/forum?id=XUIYn3jo5T">Marion et al., 2023</a>; <a class="reference external" href="https://openreview.net/forum?id=4wZiAXD29TQ">Yang et al., 2023</a>), dataset selection (<a class="reference external" href="https://openreview.net/forum?id=GC8HkKeH8s">Engstrom et al., 2024</a>; <a class="reference external" href="https://openreview.net/forum?id=HE9eUQlAvo">Chhabra et al., 2024</a>), fact tracing (<a class="reference external" href="https://aclanthology.org/2022.findings-emnlp.180">Akyurek et al., 2022</a>) and machine unlearning (<a class="reference external" href="https://arxiv.org/abs/2108.11577">Warnecke
et al., 2023</a>).</p>
<p>Although there are various demonstrations of TDA’s potential for interpretability and practical applications, the critical question of how TDA methods should be effectively evaluated remains open. Several approaches have been proposed by the community, which can be categorized into three groups:</p>
<ul class="simple">
<li><p><strong>Ground truth</strong>: As some of the methods are designed to approximate LOO effects, ground truth can often be computed for TDA evaluation. However, this counterfactual ground truth approach requires retraining the model multiple times on different subsets of the training data, which quickly becomes computationally expensive. Additionally, this ground truth is shown to be dominated by noise in practical deep learning settings, due to the inherent stochasticity of a typical training process (<a class="reference external" href="https://openreview.net/forum?id=xHKVVHGDOEk">Basu et al., 2021</a>; <a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/ca774047bc3b46cc81e53ead34cd5d5a-Abstract-Conference.html">Nguyen et al., 2023</a>).</p></li>
<li><p><strong>Downstream Task Evaluators</strong>: To remedy the challenges associated with ground truth evaluation, the literature proposes to assess the utility of a TDA method within the context of an end-task, such as model debugging or data selection (<a class="reference external" href="https://proceedings.mlr.press/v70/koh17a.html">Koh and Liang, 2017</a>; <a class="reference external" href="https://proceedings.mlr.press/v89/khanna19a.html">Khanna et al., 2019</a>; <a class="reference external" href="https://arxiv.org/abs/2111.04683">Karthikeyan et al., 2021</a>)</p></li>
<li><p><strong>Heuristics</strong>: Finally, the community also used heuristics (desirable properties or sanity checks) to evaluate the quality of TDA techniques. These include comparing the attributions of a trained model and a randomized model (<a class="reference external" href="https://openreview.net/forum?id=9uvhpyQwzM_">Hanawa et. al, 2021</a>) and measuring the amount of overlap between the attributions for different test samples (<a class="reference external" href="http://proceedings.mlr.press/v108/barshan20a/barshan20a.pdf">Barshan et al., 2020</a>).</p></li>
</ul>
<p><strong>quanda</strong> is designed to meet the need of a comprehensive and systematic evaluation framework, allowing practitioners and researchers to obtain a detailed view of the performance of TDA methods in various contexts.</p>
<section id="library-features">
<h2>Library Features<a class="headerlink" href="#library-features" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Unified TDA Interface</strong>: <strong>quanda</strong> provides a unified interface for various TDA methods, allowing users to easily switch between different methods.</p></li>
<li><p><strong>Metrics</strong>: <strong>quanda</strong> provides a set of metrics to evaluate the effectiveness of TDA methods. These metrics are based on the latest research in the field.</p></li>
<li><p><strong>Benchmarking</strong>: <strong>quanda</strong> provides a benchmarking tool to evaluate the performance of TDA methods on a given model, dataset and problem. As many TDA evaluation methods require access to ground truth, our benchmarking tools allow to generate a controlled setting with ground truth, and then compare the performance of different TDA methods on this setting.</p></li>
</ul>
</section>
<section id="supported-tda-methods">
<h2>Supported TDA Methods<a class="headerlink" href="#supported-tda-methods" title="Link to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Repository</p></th>
<th class="head"><p>Reference</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Similarity Influence</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/captum/tree/master">Captum</a></p></td>
<td><p><a class="reference external" href="https://captum.ai/api/influence.html#similarityinfluence">Caruana et al., 1999</a></p></td>
</tr>
<tr class="row-odd"><td><p>Arnoldi Influence Functions</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/captum/tree/master">Captum</a></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2112.03052">Schioppa et al., 2022</a></p></td>
</tr>
<tr class="row-even"><td><p>TracIn</p></td>
<td><p><a class="reference external" href="https://github.com/pytorch/captum/tree/master">Captum</a></p></td>
<td><p><a class="reference external" href="https://proceedings.neurips.cc/paper/2020/hash/e6385d39ec9394f2f3a354d9d2b88eec-Abstract.html">Pruthi et al., 2020</a></p></td>
</tr>
<tr class="row-odd"><td><p>Representer Point Selection</p></td>
<td><p><a class="reference external" href="https://github.com/chihkuanyeh/Representer_Point_Selection">Representer Point Selection</a></p></td>
<td><p><a class="reference external" href="https://proceedings.neurips.cc/paper/2018/hash/8a7129b8f3edd95b7d969dfc2c8e9d9d-Abstract.html">Yeh et al., 2018</a></p></td>
</tr>
<tr class="row-even"><td><p>TRAK</p></td>
<td><p><a class="reference external" href="https://github.com/MadryLab/trak">TRAK</a></p></td>
<td><p><a class="reference external" href="https://proceedings.mlr.press/v202/park23c.html">Park et al., 2023</a></p></td>
</tr>
</tbody>
</table>
</section>
<section id="evaluation-metrics">
<h2>Evaluation Metrics<a class="headerlink" href="#evaluation-metrics" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Linear Datamodeling Score</strong> (<a class="reference external" href="https://proceedings.mlr.press/v202/park23c.html">Park et al., 2023</a>): Measures the correlation between the (grouped) attribution scores and the actual output of models trained on different subsets of the training set. For each subset, the linear datamodeling score compares the actual model output with the sum of attribution scores from the subset using Spearman rank correlation.</p></li>
<li><p><strong>Identical Class / Identical Subclass</strong> (<a class="reference external" href="https://openreview.net/forum?id=9uvhpyQwzM_">Hanawa et al., 2021</a>): Measures the proportion of identical classes or subclasses in the top-1 training samples over the test dataset. If the attributions are based on similarity, they are expected to be predictive of the class of the test datapoint, as well as different subclasses under a single label.</p></li>
<li><p><strong>Top-K Cardinality</strong>  (<a class="reference external" href="http://proceedings.mlr.press/v108/barshan20a/barshan20a.pdf">Barshan et al., 2020</a>): Measures the cardinality of the union of the top-K training samples. Since the attributions are expected to be dependent on the test input, they are expected to vary heavily for different test points, resulting in a low overlap (high metric value).</p></li>
<li><p><strong>Model Randomization</strong> (<a class="reference external" href="https://openreview.net/forum?id=9uvhpyQwzM_">Hanawa et al., 2021</a>): Measures the correlation between the original TDA and the TDA of a model with randomized weights. Since the attributions are expected to depend on model parameters, the correlation between original and randomized attributions should be low.</p></li>
<li><p><strong>Data Cleaning</strong> (<a class="reference external" href="https://proceedings.mlr.press/v89/khanna19a.html">Khanna et al., 2019</a>): Uses TDA to identify training samples responsible for misclassification. Removing them from the training set, we expect to see an improvement in the model performance when we retrain the model.</p></li>
<li><p><strong>Mislabeled Data Detection</strong> (<a class="reference external" href="https://proceedings.mlr.press/v70/koh17a.html">Koh and Liang, 2017</a>): Computes the proportion of noisy training labels detected as a function of the percentage of inspected training samples. The samples are inspected in order according to their global TDA ranking, which is computed using local attributions. This produces a cumulative mislabeling detection curve. We expect to see a curve that rapidly increases as we check more of the training data, thus we compute the area under this curve.</p></li>
</ul>
</section>
<section id="benchmarks">
<h2>Benchmarks<a class="headerlink" href="#benchmarks" title="Link to this heading"></a></h2>
<p><strong>quanda</strong> comes with a few pre-computed benchmarks that can be conveniently used for evaluation in a plug-and-play manner. We are planning to significantly expand the number of benchmarks in the future. The benchmarks currently use the MNIST dataset to conduct evaluations. The following benchmarks are available:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Metric</p></th>
<th class="head"><p>Type</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>mnist_top_k_cardinality</p></td>
<td><p><a class="reference external" href="docs_api/quanda.metrics.heuristics.top_k_cardinality.html">TopKCardinalityMetric</a></p></td>
<td><p>Heuristic</p></td>
</tr>
<tr class="row-odd"><td><p>mnist_mixed_datasets</p></td>
<td><p><a class="reference external" href="docs_api/quanda.metrics.heuristics.mixed_datasets.html">MixedDatasetMetric</a></p></td>
<td><p>Heuristic</p></td>
</tr>
<tr class="row-even"><td><p>mnist_class_detection</p></td>
<td><p><a class="reference external" href="docs_api/quanda.metrics.downstream_eval.class_detection.html">ClassDetectionMetric</a></p></td>
<td><p>Downstream-Task-Evaluator</p></td>
</tr>
<tr class="row-odd"><td><p>mnist_subclass_detection</p></td>
<td><p><a class="reference external" href="docs_api/quanda.metrics.downstream_eval.subclass_detection.html">SubclassDetectionMetric</a></p></td>
<td><p>Downstream-Task-Evaluator</p></td>
</tr>
<tr class="row-even"><td><p>mnist_mislabeling_detection</p></td>
<td><p><a class="reference external" href="docs_api/quanda.metrics.downstream_eval.mislabeling_detection.html">MislabelingDetectionMetric</a></p></td>
<td><p>Downstream-Task-Evaluator</p></td>
</tr>
<tr class="row-odd"><td><p>mnist_shortcut_detection</p></td>
<td><p><a class="reference external" href="docs_api/quanda.metrics.downstream_eval.shortcut_detection.html">ShortcutDetectionMetric</a></p></td>
<td><p>Downstream-Task-Evaluator</p></td>
</tr>
<tr class="row-even"><td><p>mnist_linear_datamodeling_score</p></td>
<td><p><a class="reference external" href="docs_api/quanda.metrics.ground_truth.linear_datamodeling.html">LinearDatamodelingMetric</a></p></td>
<td><p>Ground Truth</p></td>
</tr>
</tbody>
</table>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#installation">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#basic-usage">Basic Usage</a></li>
</ul>
</li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="docs_api/modules.html">quanda</a></li>
</ul>
</div>
</section>
</section>
<section id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Link to this heading"></a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></p></li>
</ul>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="getting_started.html" class="btn btn-neutral float-right" title="Getting Started" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Dilya, Galip.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>